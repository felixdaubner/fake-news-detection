{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Fake News Detection\n",
    "\n",
    "By Felix Daubner - Hochschule der Medien\n",
    "\n",
    "Module 'Supervised and Unsupervised Learning' - Prof. Dr.-Ing. Johannes Maucher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in [Problem Understanding](01_problem-understanding.ipynb) the training data is scraped from [POLITIFACT.com](https://www.politifact.com/). [POLITIFACT.com](https://www.politifact.com/) is a project run by non-profit organization Poynter Institue for Media Studies. Via its website POLITIFACT, it verifies the truth of political statements in the USA. It originated as an election-year project of the Tampa Bay Times. Since then it rates political statements into different truth categories. The statements originate from politicans, members of the US congress, White House and lobbyists. Every statement is reviewed and then classified into a category of either \"true\", \"mostly true\", \"half true\", \"mostly false\", \"false\", and \"pants on fire\". The rating into one of these classification is done by intensive research using the information which was public the day the statement was made. \n",
    "\n",
    "[2]\n",
    "\n",
    "For the scope of this project, the website is scraped to retrieve the latest political statements which are then used to train the fake-news and sentiment model. As the model should only be trained on the statements itself, not including metadata such as speaker, date or channel, only the necessary data is scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the statements, a class \"Scraping\" is defined. Scraping contains all methods to access POLITIFACT.com and scrape the statements including truth factor and issue. All these information are saved in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Creates an object of class Scraping.\n",
    "        '''\n",
    "        self.__url = \"https://www.politifact.com/factchecks/list/?page={page}&category={category}\"\n",
    "        self.__data = pd.DataFrame(columns=[\"statement\", \"issue\", \"truth\"])\n",
    "        self.__issues = self.__getIssues()\n",
    "\n",
    "    def __getIssues(self):\n",
    "        '''\n",
    "        Scrapes POLITIFACT.com to retrieve all issues statements were made in. The scraped issues are then used to scrape up to 150 statements from every issue.\n",
    "        Returns a list of strings, the strings being the issues which can be put into the URL.\n",
    "        '''\n",
    "        results = list()\n",
    "\n",
    "        url_issues = \"https://www.politifact.com/issues/\"\n",
    "        response = requests.get(url_issues)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            divs = soup.find_all(\"div\", class_=\"c-chyron__value\")\n",
    "            for div in divs:\n",
    "                links = div.find_all(\"a\", href=True)\n",
    "                for link in links:\n",
    "                    results.append(link[\"href\"].replace(\"/\",\"\"))\n",
    "\n",
    "            return results\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Error! Aborted with status code: {response.status_code}\")\n",
    "            raise ConnectionRefusedError\n",
    "\n",
    "    def scrape(self):\n",
    "        '''\n",
    "        For every issue, five pages each containing 30 statements are scraped and then added to the existing DataFrame.\n",
    "        '''\n",
    "        for issue in self.__issues:\n",
    "            for i in range(1,6):\n",
    "                \n",
    "                response = requests.get(self.__url.format(page=i, category=issue))\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                    statement = self.__getQuote(soup)\n",
    "                    truth = self.__getTruth(soup)\n",
    "                \n",
    "                    if len(statement) !=  0 and len(truth) != 0:\n",
    "                        self.__addToDataFrame(statement, issue, truth)\n",
    "\n",
    "                time.sleep(0.01)\n",
    "    \n",
    "    def __getQuote(self, soup):\n",
    "        '''\n",
    "        Searches the BeautifulSoup object for statements which can be retrieved and returned as a list.\n",
    "        '''\n",
    "        results = list()\n",
    "        quotes = soup.find_all(\"div\", class_=\"m-statement__quote\")\n",
    "        for quote in quotes:\n",
    "            try:\n",
    "                results.append(quote.text.strip())\n",
    "            except:\n",
    "                results.append(None)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __getTruth(self, soup):\n",
    "        '''\n",
    "        Searches the BeautifulSoup object for truth classifications to retrieve and return them as a list.\n",
    "        '''\n",
    "        results = list()\n",
    "        meter = soup.find_all(\"div\", class_=\"m-statement__meter\")\n",
    "        for m in meter:\n",
    "            true = m.find_all(\"img\", class_=\"c-image__thumb\")\n",
    "            for tr in true:\n",
    "                try:\n",
    "                    results.append(tr.get(\"alt\").strip())\n",
    "                except:\n",
    "                    results.append(None)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __addToDataFrame(self, st, iss, tr):\n",
    "        '''\n",
    "        Creates a DataFrame from the given lists and concatenates the created DataFrame to the existing DataFrame. \n",
    "        '''\n",
    "        new = pd.DataFrame({\"statement\": st, \"issue\": iss, \"truth\": tr})\n",
    "        try:\n",
    "            self.__data = pd.concat([self.__data, new], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Data:\\t\\tnew\")\n",
    "            print(e)\n",
    "\n",
    "    def getData(self):\n",
    "        '''\n",
    "        Returns the DataFrame.\n",
    "        '''\n",
    "        return self.__data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object of class \"Scraping\" is created and then the data is scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = Scraping()\n",
    "scrape.scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the scraped data should be returned. Currently, the data is still in a kind of raw format which means no pre-processing was done yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = scrape.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>issue</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says Sen. Bob Casey, D-Pa., “is trying to chan...</td>\n",
       "      <td>2024-senate-elections</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Says the election results are suspicious becau...</td>\n",
       "      <td>2024-senate-elections</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A “ballot dump” around 4 a.m. in Milwaukee sho...</td>\n",
       "      <td>2024-senate-elections</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Kari Lake is threatening Social Security and ...</td>\n",
       "      <td>2024-senate-elections</td>\n",
       "      <td>half-true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican Senate candidate Sam Brown “wants t...</td>\n",
       "      <td>2024-senate-elections</td>\n",
       "      <td>half-true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement                  issue  \\\n",
       "0  Says Sen. Bob Casey, D-Pa., “is trying to chan...  2024-senate-elections   \n",
       "1  Says the election results are suspicious becau...  2024-senate-elections   \n",
       "2  A “ballot dump” around 4 a.m. in Milwaukee sho...  2024-senate-elections   \n",
       "3  “Kari Lake is threatening Social Security and ...  2024-senate-elections   \n",
       "4  Republican Senate candidate Sam Brown “wants t...  2024-senate-elections   \n",
       "\n",
       "        truth  \n",
       "0       false  \n",
       "1       false  \n",
       "2  pants-fire  \n",
       "3   half-true  \n",
       "4   half-true  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should now be saved in a csv-file. This file lays the fundation for the coming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data.to_csv(\"data/scraped.csv\", header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Test Data from LIAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded LIAR dataset should be used only as a validation & test dataset as described in [Problem Understanding](01_problem-understanding.ipynb). The LIAR dataset consists of three tsv-files which are already labeled as train, test and validation data. For the purposes of this project those three datasets should be joined to only one dataset which will then be used after training, evaluation and optimization of the fake-news and sentiment model to test the performance of both models.\n",
    "\n",
    "Firstly, all three datasets are imported into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/LIAR/train.tsv\", header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/LIAR/test.tsv\", header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv(\"data/LIAR/valid.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed that all datasets have the same number of columns and are ordered in the same way.\n",
    "For control purposes, the shapes of all datasets are printed out as well as the sum of all rows of all three datasets. This number is used to later control whether the join of the datasets worked correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train:\t(10240, 14)\n",
      "Shape test:\t(1267, 14)\n",
      "Shape valid:\t(1284, 14)\n",
      "Sum of rows:\t12791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape train:\\t{train.shape}\")\n",
    "print(f\"Shape test:\\t{test.shape}\")\n",
    "print(f\"Shape valid:\\t{valid.shape}\")\n",
    "\n",
    "print(f\"Sum of rows:\\t{train.shape[0]+test.shape[0]+valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets are joined vertically using the 'concat' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR = pd.concat([train, test, valid], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is one DataFrame with a shape of 14 columns such as all the original datasets. The number of rows matches the sum of the rows from the earlier output. So we can be assume that the data was concatenated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12791, 14)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIAR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the concatenated data is saved as csv-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR.to_csv(\"data/LIAR.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] https://www.politifact.com/article/2018/feb/12/principles-truth-o-meter-politifacts-methodology-i/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
