{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Fake News Detection\n",
    "\n",
    "By Felix Daubner - Hochschule der Medien\n",
    "\n",
    "Module 'Supervised and Unsupervised Learning' - Prof. Dr.-Ing. Johannes Maucher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, Flatten, MaxPooling1D, Dropout, Bidirectional, Input, Concatenate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "\n",
    "NUM_WORDS=3000\n",
    "MAX_SEQUENCE_LEN = 57\n",
    "NUM_CAT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareFeatures(X):\n",
    "    '''\n",
    "    This function gets the features and modifies the features in a way to be able to train neural network\n",
    "    using encoded categorical data and tokenized text.\n",
    "    Returns numpy arrays.\n",
    "    '''\n",
    "    X_token = np.array(X[\"token\"].apply(np.asarray))\n",
    "    X_token = np.array([arr for arr in X_token])\n",
    "\n",
    "    X_enc = np.array(X.drop([\"token\"], axis=1).apply(np.array))\n",
    "\n",
    "    return X_token, X_enc\n",
    "\n",
    "def prepareTarget(y):\n",
    "    '''\n",
    "    This function returns the target data as a numpy array.\n",
    "    '''\n",
    "    return np.array(y)\n",
    "\n",
    "def visualizeHistory(history):\n",
    "    '''\n",
    "    This function gets keras.History object and plots loss, validation loss, precision and validation precision.\n",
    "    Returns altair hconcat object containing the charts.\n",
    "    '''\n",
    "\n",
    "    l, p, v_l, v_p = history.history.keys()\n",
    "\n",
    "    data = pd.DataFrame({\"epoch\": history.epoch,\n",
    "            \"loss\": history.history[l],\n",
    "            \"val_loss\": history.history[v_l],\n",
    "            \"precision\": history.history[p],\n",
    "            \"val_precision\": history.history[v_p]})\n",
    "    \n",
    "    loss_min = min(data[\"loss\"].min(), data[\"val_loss\"].min())\n",
    "    loss_max = max(data[\"loss\"].max(), data[\"val_loss\"].max())\n",
    "\n",
    "    precision_min = min(data[\"precision\"].min(), data[\"val_precision\"].min())\n",
    "    precision_max = max(data[\"precision\"].max(), data[\"val_precision\"].max())\n",
    "\n",
    "    data_melted = data.melt('epoch', value_vars=['loss', 'val_loss', 'precision', 'val_precision'], var_name='type', value_name='value')\n",
    "    \n",
    "    data_loss = data_melted[data_melted[\"type\"].isin([\"loss\", \"val_loss\"])]\n",
    "    loss = alt.Chart(data_loss).mark_line().encode(\n",
    "        x = \"epoch\",\n",
    "        y = alt.Y(\"value\", scale = alt.Scale(domain=[loss_min, loss_max])),\n",
    "        color = alt.Color(\"type\", legend=alt.Legend(orient=\"right\"))\n",
    "    ).properties(\n",
    "        title = \"Training and Validation Loss over epochs\"\n",
    "    )\n",
    "\n",
    "    data_precision = data_melted[data_melted[\"type\"].isin([\"precision\", \"val_precision\"])]\n",
    "    precision = alt.Chart(data_precision).mark_line().encode(\n",
    "        x = \"epoch\",\n",
    "        y = alt.Y(\"value\", scale = alt.Scale(domain=[precision_min, precision_max])),\n",
    "        color = alt.Color(\"type\", legend=alt.Legend(orient=\"right\"))\n",
    "    ).properties(\n",
    "        title = \"Training and Validation Precision over epochs\"\n",
    "    )\n",
    "\n",
    "    return alt.hconcat(loss, precision).resolve_scale(color=\"independent\")\n",
    "\n",
    "\n",
    "def performanceReport(model, X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    This function gets a model, training and validation data.\n",
    "    It predicts training and validation data, compares it to the true data and prints out classification report for both, training and validation.\n",
    "    '''\n",
    "    y_pred_train = (model.predict(X_train) > 0.5).astype(int)\n",
    "    y_pred_val = (model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "    print(\"\\nClassifcation Report of Performance on Training data\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"* \"*10)\n",
    "\n",
    "    print(\"\\nClassifcation Report of Performance on Validation data\")\n",
    "    print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the model definition and training. Different types of models should be trained and then compared in their performances to find out which model fits the challenge, to determine whether a political statement was fake-news or true, best. \n",
    "\n",
    "There are four types of models to be compared: a feedforward neural network, a Long Short-Term Memory, a bidirectional Long Short-Term Memory and Convolutional Neural Network. Those models will vary in terms of layers and hyperparameters still trying to keep them rather simple. All models are trained using the encoded categorical data of 'channel' and the tokenized statements including stop words. All models are then trained using 20 epochs and a batch size of 128. \n",
    "\n",
    "The best model is evaluated based on training and validation performance. At the end, the best model in terms of complexity and performance is chosen and will be optimized further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training and validation data is the same for every model, the preparation of the preprocessed data resulting in a structure able to train different kinds of neural networks, is only needed to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data/processed.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['statement', 'channel_Instagram', 'channel_Other', 'channel_TV',\n",
       "       'channel_TikTok', 'channel_X', 'channel_ad', 'channel_article',\n",
       "       'channel_blog', 'channel_campaign', 'channel_debate',\n",
       "       'channel_interview', 'channel_lecture', 'channel_mail',\n",
       "       'channel_podcast', 'channel_presentation', 'channel_press',\n",
       "       'channel_social media', 'channel_speech', 'channel_talk',\n",
       "       'channel_video', 'truth', 'token', 'statement_stop', 'token_stop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting defining the different models, the data is prepared for the training process. The neural networks to be trained only take numpy arrays as input. Thus, the data currently saved as a pandas DataFrame is converted in to a numpy array. In this conversion process, only \"token\", the encoded \"channel\" columns and \"truth\" are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"statement\", \"statement_stop\", \"token_stop\", \"truth\"], axis=1)\n",
    "y = data[\"truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data into features and target, the features still have to preprared for training by splitting the encoded categorical data from the tokenized statements. The tokenized text data has to be taken care of using an Embedding Layer while a Dense layer is sufficient to handle the encoded categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token, X_train_enc = prepareFeatures(X_train)\n",
    "X_val_token, X_val_enc = prepareFeatures(X_val)\n",
    "y_train = prepareTarget(y_train)\n",
    "y_val = prepareTarget(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, the data despite being prepared to fit the structure of neural networks, is not ready for training yet. The tokenized statements saved in \"X_train_token\" and \"X_val_token\" need to be transformed into an embedding matrix which assigns every word / token a vector. This vector represents the word in a multi-dimensional vector space and models the relationship between different words.\n",
    "\n",
    "A pre-trained word embedding from FastText which already contains the vectors for each word is used to create the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to remember the words which are placed behind each token, the trained 'tokenizer'-object of section [data pre-processing](03_data-understanding.ipynb) is imported using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer/tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates an embedding matrix which assigns every word their respective vector as saved in the pre-trained word embedding of FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300  \n",
    "word_index = tokenizer.word_index \n",
    "num_words = min(len(word_index) + 1, NUM_WORDS)  \n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        if word in word2vec.key_to_index:\n",
    "            embedding_vector = word2vec[word]\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every kind of model which is going to be trained in the following, two input layers are defined. Those two input layers are the same used for every kind of model as the input data doesn't vary between types of models.\n",
    "\n",
    "There is one input layer for tokenized text data and another input layer for encoded categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(MAX_SEQUENCE_LEN,), name=\"text_input\")\n",
    "categorical_input = Input(shape=(NUM_CAT,), name=\"categorical_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input layer and embedding matrix, an Embedding layer can be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(NUM_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LEN, trainable=False)(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the encoded categorical data, a dense layer is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Dense(32, activation=\"relu\")(categorical_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at fake-news detection, it is decided which metric should be optimized. Classification provide a lots of useful metrics which have to be chosen for each project individually. The most common and known metrics are accuracy, precision, recall and f1-score.\n",
    "\n",
    "Most often, accuracy is not a good metric as it doesn't take into account the cost of predicting errors. That's why either precision or recall should be used.\n",
    "\n",
    "The worst case at fake-news prediction is when a fake-news statement is not identified as fake-news but as true. Whereas the other way, a true statement being classified as fake-news statement does not harm in the same way. Translating this into the terms of this project means a false positive (\"a statement which is 'fake' (0) gets classified as 'true' (1)\") is worse than a false negative (\"a statement which is 'true' (1) gets classified as 'false' (0)\"). The metrics focusing on optimizing the false positives is precision. Therefore, precision is used when trying to chose and optimize a fake-news classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, four different types models are trained and evaluated. Based on those evaluations, the best model is chosen. \n",
    "\n",
    "All models are designed in a way so that the number of trainable parameters are in the same region to being able to compare the results of the models. The evaluations of the models after each respective training are kept short as the focus lays on the displayed metrics. In the next chapter discusses the model to keep by relying on the previously achieved metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Nerwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model to be trained is a simple feedforward neural network. The feedforward neural network consists of Dense and Dropout layers which make the architecture quite easy and not too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_flatten_text = Flatten()(emb)\n",
    "\n",
    "ff_combined = Concatenate()([ff_flatten_text, cat])\n",
    "ff_dense1 = Dense(32, activation=\"relu\")(ff_combined)\n",
    "ff_drop = Dropout(0.4)(ff_dense1)\n",
    "ff_dense2 = Dense(16, activation=\"relu\")(ff_drop)\n",
    "ff_output = Dense(1, activation=\"sigmoid\")(ff_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 17100)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 17132)        0           ['flatten[0][0]',                \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           548256      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 32)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           528         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            17          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,449,473\n",
      "Trainable params: 549,473\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff = Model(inputs=[categorical_input, text_input], outputs=ff_output)\n",
    "ff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model, the optimizer \"sgd\" (Stochastic Gradient Descent) is used. Also, for binary classification the loss function \"binary_crossentropy\" is the one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 21:04:04.458838: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 5ms/step - loss: 0.6926 - precision: 0.5160 - val_loss: 0.6891 - val_precision: 0.5240\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.6865 - precision: 0.5570 - val_loss: 0.6826 - val_precision: 0.5511\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6793 - precision: 0.5811 - val_loss: 0.6757 - val_precision: 0.5670\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6729 - precision: 0.5989 - val_loss: 0.6696 - val_precision: 0.5767\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6677 - precision: 0.6112 - val_loss: 0.6646 - val_precision: 0.5843\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6630 - precision: 0.6182 - val_loss: 0.6580 - val_precision: 0.6000\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.6575 - precision: 0.6315 - val_loss: 0.6519 - val_precision: 0.6142\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.6512 - precision: 0.6450 - val_loss: 0.6449 - val_precision: 0.6270\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6454 - precision: 0.6544 - val_loss: 0.6396 - val_precision: 0.6304\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6362 - precision: 0.6641 - val_loss: 0.6303 - val_precision: 0.6675\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6299 - precision: 0.6758 - val_loss: 0.6238 - val_precision: 0.6599\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6201 - precision: 0.6848 - val_loss: 0.6173 - val_precision: 0.6545\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.6129 - precision: 0.6876 - val_loss: 0.6089 - val_precision: 0.6886\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 0.6040 - precision: 0.7015 - val_loss: 0.6029 - val_precision: 0.6661\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.5982 - precision: 0.6989 - val_loss: 0.5972 - val_precision: 0.6638\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 1s 5ms/step - loss: 0.5887 - precision: 0.7080 - val_loss: 0.5916 - val_precision: 0.6652\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.5791 - precision: 0.7183 - val_loss: 0.5815 - val_precision: 0.7003\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.5722 - precision: 0.7260 - val_loss: 0.5746 - val_precision: 0.7163\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.5631 - precision: 0.7321 - val_loss: 0.5676 - val_precision: 0.7038\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 0s 4ms/step - loss: 0.5552 - precision: 0.7365 - val_loss: 0.5617 - val_precision: 0.7112\n"
     ]
    }
   ],
   "source": [
    "ff_hist = ff.fit([X_train_enc, X_train_token], y_train, epochs=20, batch_size=128, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the feedforward neural network, there are some things standing out.\n",
    "\n",
    "As seen in the visualizatons below, both the training and validation loss decline per epoch. Same goes for precision which inclines per epoch. Both metrics show small signs of overfitting as the training metrics perform increasingly better compared to the validation metrics per epoch. Overfitting should be avoided as the performance on new, unseen data will be significantly worse than the performance on training data.\n",
    "\n",
    "Also by using more epochs, the model can still be improved. Both loss and precision seem like with more epochs their values could still improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-c8138b89c63b4a19913b388ad02b614a.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-c8138b89c63b4a19913b388ad02b614a.vega-embed details,\n",
       "  #altair-viz-c8138b89c63b4a19913b388ad02b614a.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-c8138b89c63b4a19913b388ad02b614a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c8138b89c63b4a19913b388ad02b614a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c8138b89c63b4a19913b388ad02b614a\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-877b5cd90e22d9a0b7cf89e0ca0214b9\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5551830530166626, 0.6925581097602844]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-52a799254bc179129bdb36a3d602f30b\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5160066485404968, 0.736510157585144]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-877b5cd90e22d9a0b7cf89e0ca0214b9\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6925581097602844}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6865132451057434}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6792587041854858}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.672940731048584}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6676588654518127}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6629725694656372}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6575263142585754}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6511968374252319}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6454119086265564}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6361990571022034}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.629867672920227}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6201170086860657}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6129247546195984}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6040442585945129}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.5981580018997192}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.5887470245361328}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.5790628790855408}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.5722102522850037}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.5630675554275513}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.5551830530166626}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6890678405761719}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6825674176216125}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6757238507270813}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6696401834487915}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6645841002464294}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6580379605293274}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.6518661379814148}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6449394226074219}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6395847797393799}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6303490996360779}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6237833499908447}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6172813773155212}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6089048981666565}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6028562784194946}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.5971980094909668}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.5916045308113098}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.581462025642395}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.5746433734893799}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.5676169991493225}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.5616674423217773}], \"data-52a799254bc179129bdb36a3d602f30b\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5160066485404968}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5570047497749329}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5810686349868774}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5989041328430176}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.61124587059021}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.6181609630584717}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.6315096616744995}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6449583172798157}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6544449329376221}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6640713214874268}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.67579185962677}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6847779750823975}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.6875529289245605}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.7015225291252136}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.6989337801933289}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.7079671025276184}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.7183455228805542}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.7259635925292969}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.7320594191551208}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.736510157585144}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.523973286151886}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.551120936870575}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5669729709625244}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5767428278923035}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.5842506289482117}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.6000000238418579}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.6141955852508545}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.626991868019104}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.6303834915161133}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.6675422191619873}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.6599348783493042}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.6545175909996033}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6885592937469482}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6660561561584473}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.663772463798523}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.6651995182037354}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.7003047466278076}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.7163094878196716}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.703825831413269}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.7111707329750061}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(ff_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the results of the classification report, the signs of overfitting can not be validated. The model performs already quite good with a weighted precision on the training data of 0.77, the weighted precision on the validation data is at 0.72. Due to the simple architecture of the model, the feedforward neural networks should considered when evaluating the best model. Though, as Feedforward Neural Network can not comprehend sequential data like texts, it should be discussed whether despsite the good results this model is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 0s 838us/step\n",
      "190/190 [==============================] - 0s 786us/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76      6972\n",
      "           1       0.77      0.78      0.77      7144\n",
      "\n",
      "    accuracy                           0.77     14116\n",
      "   macro avg       0.77      0.77      0.77     14116\n",
      "weighted avg       0.77      0.77      0.77     14116\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      3111\n",
      "           1       0.71      0.72      0.72      2939\n",
      "\n",
      "    accuracy                           0.72      6050\n",
      "   macro avg       0.72      0.72      0.72      6050\n",
      "weighted avg       0.72      0.72      0.72      6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(ff, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next type of model is a Long Short-Term Memory Neural Network. This type of neural network is more complex than a Feedforward Neural Networks as it can capture contextual information making it useful for sequential data like texts or time-sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ = LSTM(256)(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_combined = Concatenate()([lstm_, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dense1 = Dense(128, activation='relu')(lstm_combined)\n",
    "lstm_drop1 = Dropout(0.4)(lstm_dense1)\n",
    "lstm_dense2 = Dense(64, activation='relu')(lstm_drop1)\n",
    "lstm_output = Dense(1, activation='sigmoid')(lstm_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          570368      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 288)          0           ['lstm[0][0]',                   \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          36992       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,516,353\n",
      "Trainable params: 616,353\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = Model(inputs=[categorical_input, text_input], outputs=lstm_output)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 21s 177ms/step - loss: 0.6906 - precision_1: 0.5436 - val_loss: 0.6892 - val_precision_1: 0.5288\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 20s 176ms/step - loss: 0.6884 - precision_1: 0.5525 - val_loss: 0.6864 - val_precision_1: 0.5644\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 20s 177ms/step - loss: 0.6862 - precision_1: 0.5761 - val_loss: 0.6832 - val_precision_1: 0.5891\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 19s 172ms/step - loss: 0.6837 - precision_1: 0.5907 - val_loss: 0.6802 - val_precision_1: 0.6005\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6808 - precision_1: 0.6041 - val_loss: 0.6768 - val_precision_1: 0.6093\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 20s 179ms/step - loss: 0.6778 - precision_1: 0.6178 - val_loss: 0.6731 - val_precision_1: 0.6138\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6752 - precision_1: 0.6116 - val_loss: 0.6683 - val_precision_1: 0.6355\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6712 - precision_1: 0.6250 - val_loss: 0.6640 - val_precision_1: 0.6259\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 20s 177ms/step - loss: 0.6659 - precision_1: 0.6314 - val_loss: 0.6581 - val_precision_1: 0.6352\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 20s 184ms/step - loss: 0.6621 - precision_1: 0.6328 - val_loss: 0.6521 - val_precision_1: 0.6366\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 21s 188ms/step - loss: 0.6564 - precision_1: 0.6351 - val_loss: 0.6461 - val_precision_1: 0.6300\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6501 - precision_1: 0.6428 - val_loss: 0.6385 - val_precision_1: 0.6397\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 21s 188ms/step - loss: 0.6452 - precision_1: 0.6431 - val_loss: 0.6328 - val_precision_1: 0.6274\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 21s 193ms/step - loss: 0.6385 - precision_1: 0.6463 - val_loss: 0.6242 - val_precision_1: 0.6408\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 20s 179ms/step - loss: 0.6317 - precision_1: 0.6490 - val_loss: 0.6170 - val_precision_1: 0.6440\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 20s 184ms/step - loss: 0.6260 - precision_1: 0.6525 - val_loss: 0.6091 - val_precision_1: 0.6569\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 20s 181ms/step - loss: 0.6193 - precision_1: 0.6566 - val_loss: 0.6029 - val_precision_1: 0.6590\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6133 - precision_1: 0.6596 - val_loss: 0.5971 - val_precision_1: 0.6623\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 21s 187ms/step - loss: 0.6084 - precision_1: 0.6602 - val_loss: 0.5931 - val_precision_1: 0.6538\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 22s 198ms/step - loss: 0.6050 - precision_1: 0.6632 - val_loss: 0.5886 - val_precision_1: 0.6549\n"
     ]
    }
   ],
   "source": [
    "lstm_hist = lstm.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the LSTM Neural Network differ to the results of the Feedforward Neural Network. As seen in the Loss over Epochs plot, the validation loss is actually lower than the loss on the training data for all epochs. The precision on the training data is also lower for first 6 epochs. After a strong increase in precision in the first 6 epochs the increase per epoch over the remaining 14 epochs decreases.\n",
    "\n",
    "There are no signs of overfitting as the training and validation move on equal levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2de5ff07ea1948e49d9465ceffccab0d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2de5ff07ea1948e49d9465ceffccab0d.vega-embed details,\n",
       "  #altair-viz-2de5ff07ea1948e49d9465ceffccab0d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2de5ff07ea1948e49d9465ceffccab0d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2de5ff07ea1948e49d9465ceffccab0d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2de5ff07ea1948e49d9465ceffccab0d\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-e7a625fef7bc679945fc01ab23520acc\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5885763764381409, 0.6905575394630432]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-582f6696ddfbba7993a6d4d0821f17f3\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.528799831867218, 0.6632311344146729]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-e7a625fef7bc679945fc01ab23520acc\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6905575394630432}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6884415149688721}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6861515045166016}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6837292313575745}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6808339953422546}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6778416633605957}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6751975417137146}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6712497472763062}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6658620238304138}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6620561480522156}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6564080119132996}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6500703692436218}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6452360153198242}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6385154724121094}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.631723940372467}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6260092258453369}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6193280220031738}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.613343358039856}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6084299683570862}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.6049959659576416}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6891622543334961}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6863626837730408}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6832104921340942}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6801530122756958}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6767857670783997}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6731083989143372}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.6683034300804138}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.663984477519989}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6580943465232849}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6520972847938538}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6460777521133423}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6385037899017334}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6328303217887878}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6241941452026367}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6170419454574585}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.609110951423645}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.602868914604187}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.5970514416694641}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.5931441187858582}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.5885763764381409}], \"data-582f6696ddfbba7993a6d4d0821f17f3\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5436400175094604}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5525458455085754}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5761488080024719}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5907236933708191}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.6040830612182617}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.6177864074707031}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.6116493940353394}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6249683499336243}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6314383149147034}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6327683329582214}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.6351200342178345}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6428220868110657}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.6430718898773193}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.6462942361831665}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.648952305316925}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6525111198425293}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6566334962844849}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6595525741577148}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6602128744125366}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6632311344146729}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.528799831867218}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5643670558929443}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5891238451004028}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.6005246639251709}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.6093172430992126}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.6138392686843872}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.635465145111084}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.6259384155273438}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.6351575255393982}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.6365935802459717}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.6300211548805237}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.6396874189376831}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6274459362030029}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6408411860466003}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.6439732313156128}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.6569115519523621}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.6589743494987488}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6623460650444031}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.6537572145462036}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6549437642097473}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(lstm_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classficiation reports show that training and validation data perform on an equal level, the validation even performing slightly better. The weighted precision value of the training data is at 0.69 which is worse compared to the feedforward neural network. Even though, a LSTM has a more complex architecture it is designed to specifically handle sequential input like texts. The performance compared to the Feedforward Neural Network might be worse, but not in a way which can not be fixed by optimizing hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 15s 33ms/step\n",
      "190/190 [==============================] - 6s 31ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.59      0.65      6972\n",
      "           1       0.66      0.78      0.72      7144\n",
      "\n",
      "    accuracy                           0.69     14116\n",
      "   macro avg       0.69      0.69      0.69     14116\n",
      "weighted avg       0.69      0.69      0.69     14116\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.62      0.67      3111\n",
      "           1       0.65      0.77      0.71      2939\n",
      "\n",
      "    accuracy                           0.69      6050\n",
      "   macro avg       0.70      0.69      0.69      6050\n",
      "weighted avg       0.70      0.69      0.69      6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(lstm, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bidirectional LSTM compared to a standard LSTM processes the input in both forward and backward directions. This allows the model to capture past and future context at a time. The architecture againg being more complex to a LSTM and Feedforward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_ = Bidirectional(LSTM(128))(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_combined = Concatenate()([blstm_, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_dense1 = Dense(128, activation='relu')(blstm_combined)\n",
    "blstm_drop1 = Dropout(0.4)(blstm_dense1)\n",
    "blstm_dense2 = Dense(32, activation='relu')(blstm_drop1)\n",
    "blstm_output = Dense(1, activation='sigmoid')(blstm_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 256)          439296      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 288)          0           ['bidirectional[0][0]',          \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          36992       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           4128        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,381,121\n",
      "Trainable params: 481,121\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm = Model(inputs=[categorical_input, text_input], outputs=blstm_output)\n",
    "blstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 16s 127ms/step - loss: 0.6903 - precision_2: 0.5211 - val_loss: 0.6879 - val_precision_2: 0.5453\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6864 - precision_2: 0.5579 - val_loss: 0.6833 - val_precision_2: 0.5773\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 14s 126ms/step - loss: 0.6829 - precision_2: 0.5828 - val_loss: 0.6795 - val_precision_2: 0.5845\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 15s 134ms/step - loss: 0.6793 - precision_2: 0.5922 - val_loss: 0.6756 - val_precision_2: 0.5902\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 15s 131ms/step - loss: 0.6755 - precision_2: 0.5971 - val_loss: 0.6713 - val_precision_2: 0.5925\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 15s 139ms/step - loss: 0.6720 - precision_2: 0.6068 - val_loss: 0.6667 - val_precision_2: 0.5959\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6680 - precision_2: 0.6103 - val_loss: 0.6621 - val_precision_2: 0.6000\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 15s 133ms/step - loss: 0.6635 - precision_2: 0.6111 - val_loss: 0.6568 - val_precision_2: 0.6105\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 14s 125ms/step - loss: 0.6597 - precision_2: 0.6160 - val_loss: 0.6515 - val_precision_2: 0.6132\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 14s 127ms/step - loss: 0.6561 - precision_2: 0.6171 - val_loss: 0.6463 - val_precision_2: 0.6119\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 14s 126ms/step - loss: 0.6495 - precision_2: 0.6277 - val_loss: 0.6408 - val_precision_2: 0.6075\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6455 - precision_2: 0.6247 - val_loss: 0.6338 - val_precision_2: 0.6196\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6407 - precision_2: 0.6290 - val_loss: 0.6279 - val_precision_2: 0.6202\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 15s 131ms/step - loss: 0.6334 - precision_2: 0.6359 - val_loss: 0.6208 - val_precision_2: 0.6273\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 15s 135ms/step - loss: 0.6299 - precision_2: 0.6335 - val_loss: 0.6145 - val_precision_2: 0.6284\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 15s 131ms/step - loss: 0.6236 - precision_2: 0.6391 - val_loss: 0.6089 - val_precision_2: 0.6257\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 14s 127ms/step - loss: 0.6168 - precision_2: 0.6413 - val_loss: 0.6027 - val_precision_2: 0.6301\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 14s 126ms/step - loss: 0.6131 - precision_2: 0.6470 - val_loss: 0.5964 - val_precision_2: 0.6501\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 14s 129ms/step - loss: 0.6088 - precision_2: 0.6499 - val_loss: 0.5923 - val_precision_2: 0.6371\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 14s 130ms/step - loss: 0.6038 - precision_2: 0.6536 - val_loss: 0.5871 - val_precision_2: 0.6559\n"
     ]
    }
   ],
   "source": [
    "blstm_hist = blstm.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bidirectional LSTM behaves similar to the LSTM as the validation loss is lower than the loss on the training data. Both losses also diverge with increasing epochs.\n",
    "\n",
    "Also the precision curves look the same with a strong increase in the beginning and a slower increase towards the end. After 20 epochs, the precision curves meet at the same level around 0.64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-be0dfff61e07460aa371e77058e0a094.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-be0dfff61e07460aa371e77058e0a094.vega-embed details,\n",
       "  #altair-viz-be0dfff61e07460aa371e77058e0a094.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-be0dfff61e07460aa371e77058e0a094\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-be0dfff61e07460aa371e77058e0a094\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-be0dfff61e07460aa371e77058e0a094\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-b2da458bfc137e728ad03cfb214d4924\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5871179103851318, 0.690295934677124]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-011b9ab1669494b577cb5d869903f590\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5210736989974976, 0.6558716893196106]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-b2da458bfc137e728ad03cfb214d4924\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.690295934677124}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6864435076713562}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6829333305358887}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6793482899665833}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6755281090736389}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6720046997070312}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6680294871330261}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6634848117828369}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6597424149513245}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.65611732006073}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6494718194007874}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6454612612724304}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6407124400138855}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.633435845375061}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.6299045085906982}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6236101388931274}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6168103814125061}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6131078600883484}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6088396310806274}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.603763997554779}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6878515481948853}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6833091974258423}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6794700622558594}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6755759716033936}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6712625026702881}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6667307615280151}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.662052571773529}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6567830443382263}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.651489794254303}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6463233232498169}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6408441066741943}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6337937712669373}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6278725266456604}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6207679510116577}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6144636869430542}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.6089004874229431}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6027465462684631}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.5964032411575317}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.5922702550888062}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.5871179103851318}], \"data-011b9ab1669494b577cb5d869903f590\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5210736989974976}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5579027533531189}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5828044414520264}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5921873450279236}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.5971121788024902}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.606842041015625}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.6103221774101257}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6110529899597168}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6160197257995605}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6171410083770752}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.6277042031288147}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6247020363807678}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.6289752721786499}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.635865330696106}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.6335495710372925}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6390660405158997}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6412881016731262}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6469900608062744}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6498626470565796}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6535769104957581}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5452703833580017}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5773430466651917}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5845298767089844}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5902019739151001}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.5925215482711792}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.5958610773086548}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.6000000238418579}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.6104500889778137}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.6131850481033325}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.6118881106376648}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.607501208782196}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.619556725025177}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6202206611633301}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6272776126861572}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.6284036040306091}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.6257309913635254}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.6300697326660156}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6500592827796936}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.6371117830276489}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6558716893196106}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(blstm_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, by looking at the classifcation reports, the precision values of both training and validation move on the same level just like the LSTM did. As those metrics do not provide an improvement compared to the LSTM, the bidirectional LSTM is not considered for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 10s 23ms/step\n",
      "190/190 [==============================] - 4s 22ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65      6972\n",
      "           1       0.66      0.74      0.70      7144\n",
      "\n",
      "    accuracy                           0.68     14116\n",
      "   macro avg       0.68      0.68      0.68     14116\n",
      "weighted avg       0.68      0.68      0.68     14116\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67      3111\n",
      "           1       0.66      0.74      0.69      2939\n",
      "\n",
      "    accuracy                           0.68      6050\n",
      "   macro avg       0.69      0.69      0.68      6050\n",
      "weighted avg       0.69      0.68      0.68      6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(blstm, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ = Conv1D(filters=128, kernel_size=5, activation='relu')(emb)\n",
    "cnn_maxpool = MaxPooling1D(pool_size=5)(cnn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_flatten_text = Flatten()(cnn_)\n",
    "\n",
    "cnn_combined = Concatenate()([cnn_flatten_text, cat])\n",
    "cnn_flatten = Flatten()(cnn_maxpool)\n",
    "cnn_dense1 = Dense(256, activation=\"relu\")(cnn_flatten)\n",
    "cnn_drop = Dropout(0.2)(cnn_dense1)\n",
    "cnn_dense2 = Dense(128, activation=\"relu\")(cnn_drop)\n",
    "cnn_dense3 = Dense(64, activation=\"relu\")(cnn_dense2)\n",
    "cnn_output = Dense(1, activation=\"sigmoid\")(cnn_dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 53, 128)      192128      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 10, 128)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1280)         0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 256)          327936      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 128)          32896       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 64)           8256        ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            65          ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,461,281\n",
      "Trainable params: 561,281\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Model(inputs=[categorical_input, text_input], outputs=cnn_output)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 3s 26ms/step - loss: 0.6902 - precision_3: 0.5497 - val_loss: 0.6878 - val_precision_3: 0.5465\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6852 - precision_3: 0.5652 - val_loss: 0.6845 - val_precision_3: 0.5491\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6826 - precision_3: 0.5707 - val_loss: 0.6818 - val_precision_3: 0.5548\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6801 - precision_3: 0.5746 - val_loss: 0.6799 - val_precision_3: 0.5595\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6779 - precision_3: 0.5807 - val_loss: 0.6784 - val_precision_3: 0.5594\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6764 - precision_3: 0.5838 - val_loss: 0.6768 - val_precision_3: 0.5617\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6755 - precision_3: 0.5854 - val_loss: 0.6758 - val_precision_3: 0.5623\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6739 - precision_3: 0.5851 - val_loss: 0.6740 - val_precision_3: 0.5656\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6729 - precision_3: 0.5857 - val_loss: 0.6722 - val_precision_3: 0.5694\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6712 - precision_3: 0.5900 - val_loss: 0.6702 - val_precision_3: 0.5723\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6701 - precision_3: 0.5914 - val_loss: 0.6685 - val_precision_3: 0.5728\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6676 - precision_3: 0.5955 - val_loss: 0.6665 - val_precision_3: 0.5745\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6656 - precision_3: 0.6008 - val_loss: 0.6643 - val_precision_3: 0.5750\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6635 - precision_3: 0.6022 - val_loss: 0.6610 - val_precision_3: 0.5868\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6608 - precision_3: 0.6100 - val_loss: 0.6607 - val_precision_3: 0.5741\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6570 - precision_3: 0.6143 - val_loss: 0.6550 - val_precision_3: 0.5910\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6530 - precision_3: 0.6230 - val_loss: 0.6506 - val_precision_3: 0.5992\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6487 - precision_3: 0.6276 - val_loss: 0.6449 - val_precision_3: 0.6214\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 3s 26ms/step - loss: 0.6441 - precision_3: 0.6395 - val_loss: 0.6416 - val_precision_3: 0.6104\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6377 - precision_3: 0.6515 - val_loss: 0.6347 - val_precision_3: 0.6276\n"
     ]
    }
   ],
   "source": [
    "cnn_hist = cnn.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convolutional Neural Network shows similar movement in both metrics over both, training and validation data. The loss function being nearly similar, the precision values do differ over epoch even though at the end, they are on the same level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9464d92a62e24fa7ae2fce481fee3ff3.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9464d92a62e24fa7ae2fce481fee3ff3.vega-embed details,\n",
       "  #altair-viz-9464d92a62e24fa7ae2fce481fee3ff3.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9464d92a62e24fa7ae2fce481fee3ff3\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9464d92a62e24fa7ae2fce481fee3ff3\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9464d92a62e24fa7ae2fce481fee3ff3\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-44b8b053b4d667f961dcee2c1c58dcd2\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.6347055435180664, 0.6901973485946655]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-9a58f8e14f5d3f5cab8844933f7ac96f\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5465303659439087, 0.6515278816223145]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-44b8b053b4d667f961dcee2c1c58dcd2\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6901973485946655}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6852012872695923}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6825616359710693}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6800729036331177}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6779178977012634}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6763673424720764}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6755142211914062}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6739460825920105}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6729122400283813}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6711504459381104}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6701071262359619}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6675758957862854}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.665623664855957}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6634534001350403}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.6607816219329834}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6569932103157043}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6530200839042664}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6487497091293335}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6441065669059753}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.6377435922622681}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6877890229225159}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6844590306282043}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.681814968585968}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6798728108406067}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6783640384674072}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6768261790275574}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.6758434176445007}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6740289926528931}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.672171950340271}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6702220439910889}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6685000061988831}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6665240526199341}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6642793416976929}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6610086560249329}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6606897711753845}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.6549864411354065}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6505811810493469}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.6449339389801025}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.6415961384773254}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.6347055435180664}], \"data-9a58f8e14f5d3f5cab8844933f7ac96f\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5496770739555359}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5651720762252808}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5706595182418823}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5745842456817627}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.5806536078453064}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.5838173031806946}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.5853592753410339}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.5850726366043091}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.5857408046722412}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.5900429487228394}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.5914181470870972}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.595546543598175}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.6008102893829346}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.6022040247917175}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.610031247138977}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6142780780792236}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6230245232582092}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6275861859321594}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6395429968833923}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6515278816223145}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5465303659439087}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5491345524787903}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5547682046890259}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5594913959503174}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.559429943561554}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.5617321729660034}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.5623493790626526}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.565578818321228}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.5694488286972046}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.5722562074661255}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.5728185772895813}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.5745398998260498}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.5749772787094116}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.5868396162986755}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.5740585923194885}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.5910465717315674}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.5992099642753601}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6213977932929993}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.6103973984718323}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6276494860649109}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(cnn_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the CNN is even worse compared to the LSTM and Bidirectional LSTM. The precision values are at 0.66. Also due to the complex structure of the Neural Network, CNNs are not considered as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 1s 3ms/step\n",
      "190/190 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64      6972\n",
      "           1       0.65      0.69      0.67      7144\n",
      "\n",
      "    accuracy                           0.66     14116\n",
      "   macro avg       0.66      0.66      0.66     14116\n",
      "weighted avg       0.66      0.66      0.66     14116\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64      3111\n",
      "           1       0.63      0.68      0.65      2939\n",
      "\n",
      "    accuracy                           0.65      6050\n",
      "   macro avg       0.65      0.65      0.65      6050\n",
      "weighted avg       0.65      0.65      0.65      6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(cnn, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Optimization\n",
    "\n",
    "#### Random Search, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following section contains the optimization and further evaluation of the chosen model from the previous chapter. The model is optimized in hyperparameters and feature extraction. \n",
    "Currently, the models were trained using the tokenized data including stop words and the encoded categorical channel columns. Feature extraction is going to decide which features are needed to achieve the best results. \n",
    "\n",
    "Focus in optimization and feature extraction are to have less complicity combined with only using features contributing to improving a models overall performance.\n",
    "At the end, the best found model should be able to master the task of fake-news classification based on the test set which consist of the famous LIAR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the results of all types of Neural Networks which were trained in the scope of this notebook to the baseline Logistic Regression model, all Neural Networks perform significantly better than the baseline model.\n",
    "\n",
    "As a reminder, the baseline model performed at a training precision of 0.57 and validation precision of 0.56. The worst performance of the Neural Networks trained in this notebook achieve precision results of at least 0.64. This proves that simple models do not always achieve the best results. Especially with complex data such as texts or images, more complex models like Neural Networks stand out as they can understand relationship between data.\n",
    "\n",
    "Based on the intentionally short-kept evaluations of all Neural Networks, the Feedforward Neural Network performed by far the best. Still, the LSTM is taken as the best performing model and shall be optimized further. This decision was made as LSTM, instead of Feedforward Neural Networks, do have the characteristic to better handle sequential inputs. The dataset might be too small or even prepared in a way the Feedforward Neural Network can handle well. Still for further fake-news detection, a LSTM is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction should decide which features to train the model on. As some features contribute more to a models performance than others, only those features which contribute significantly should be kept. As justified above, LSTM model is kept and will be used to determine which features to keep. \n",
    "\n",
    "The current performances of the model are achieved by using the encoded categorical 'channel' as well as the tokenized text including stop-words.\n",
    "So three more configurations shall be tested: \n",
    "\n",
    "1) tokenized text including stop-words without encoded categorical 'channel'\n",
    "\n",
    "After testing this configuration, it is decided whether encoded categorical 'channel' contributes to improve the models performance. Based on this decision the next model is trained with stop-words either with or without encoded categorical 'channel'.\n",
    "\n",
    "2) tokenized text excluding stop-words without encoded categorical 'channel'\n",
    "3) tokenized text excluding stop-words and encoded categorical 'channel' \n",
    "\n",
    "Again, the model performing best is evaluated and only those features are kept for further optimization.\n",
    "\n",
    "Starting with the model only trained on the text including stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_input (InputLayer)     [(None, 57)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 57, 300)           900000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256)               570368    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,511,585\n",
      "Trainable params: 611,585\n",
      "Non-trainable params: 900,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_stop_ = LSTM(256)(emb)\n",
    "lstm_stop_dense1 = Dense(128, activation='relu')(lstm_stop_)\n",
    "lstm_stop_drop1 = Dropout(0.4)(lstm_stop_dense1)\n",
    "lstm_stop_dense2 = Dense(64, activation='relu')(lstm_stop_drop1)\n",
    "lstm_stop_output = Dense(1, activation='sigmoid')(lstm_stop_dense2)\n",
    "lstm_stop = Model(inputs=text_input, outputs=lstm_stop_output)\n",
    "lstm_stop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_stop.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 20s 173ms/step - loss: 0.6930 - precision_4: 0.5227 - val_loss: 0.6923 - val_precision_4: 0.5067\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 21s 186ms/step - loss: 0.6919 - precision_4: 0.5297 - val_loss: 0.6919 - val_precision_4: 0.5025\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 20s 178ms/step - loss: 0.6911 - precision_4: 0.5273 - val_loss: 0.6910 - val_precision_4: 0.5121\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 19s 175ms/step - loss: 0.6905 - precision_4: 0.5330 - val_loss: 0.6902 - val_precision_4: 0.5222\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 19s 172ms/step - loss: 0.6899 - precision_4: 0.5413 - val_loss: 0.6892 - val_precision_4: 0.5367\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 19s 176ms/step - loss: 0.6890 - precision_4: 0.5506 - val_loss: 0.6881 - val_precision_4: 0.5524\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.6881 - precision_4: 0.5735 - val_loss: 0.6873 - val_precision_4: 0.5416\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 19s 172ms/step - loss: 0.6868 - precision_4: 0.5669 - val_loss: 0.6858 - val_precision_4: 0.5527\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 19s 174ms/step - loss: 0.6856 - precision_4: 0.5723 - val_loss: 0.6839 - val_precision_4: 0.5775\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 20s 177ms/step - loss: 0.6839 - precision_4: 0.5814 - val_loss: 0.6819 - val_precision_4: 0.5861\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6824 - precision_4: 0.5909 - val_loss: 0.6797 - val_precision_4: 0.5897\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 20s 178ms/step - loss: 0.6807 - precision_4: 0.5919 - val_loss: 0.6773 - val_precision_4: 0.5868\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 20s 184ms/step - loss: 0.6776 - precision_4: 0.5985 - val_loss: 0.6739 - val_precision_4: 0.6007\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 21s 188ms/step - loss: 0.6751 - precision_4: 0.6036 - val_loss: 0.6702 - val_precision_4: 0.6073\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 21s 189ms/step - loss: 0.6720 - precision_4: 0.6096 - val_loss: 0.6662 - val_precision_4: 0.6050\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 20s 176ms/step - loss: 0.6682 - precision_4: 0.6124 - val_loss: 0.6617 - val_precision_4: 0.6092\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 20s 184ms/step - loss: 0.6633 - precision_4: 0.6165 - val_loss: 0.6568 - val_precision_4: 0.6115\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 20s 181ms/step - loss: 0.6594 - precision_4: 0.6220 - val_loss: 0.6526 - val_precision_4: 0.6102\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 20s 179ms/step - loss: 0.6549 - precision_4: 0.6259 - val_loss: 0.6506 - val_precision_4: 0.5977\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 21s 186ms/step - loss: 0.6512 - precision_4: 0.6263 - val_loss: 0.6441 - val_precision_4: 0.6311\n"
     ]
    }
   ],
   "source": [
    "lstm_stop_hist = lstm_stop.fit(X_train_token, y_train, batch_size=128, epochs=20, validation_data=( X_val_token, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both metrics, loss and precision, behave as expected also compared to the LSTM which was trained additionally on the encoded categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8.vega-embed details,\n",
       "  #altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-58156e044d8e4d1c8c5e0bbe669d58e8\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-51fef840f0c543c91494d72e28f5550f\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.6440920233726501, 0.692997395992279]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-2523b8777aef409d03e0e8ad909392c6\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5025289058685303, 0.6310679316520691]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-51fef840f0c543c91494d72e28f5550f\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.692997395992279}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6918618083000183}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6911019682884216}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6905450820922852}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6899305582046509}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6889597177505493}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6880900263786316}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6868440508842468}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.685580849647522}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6839316487312317}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6824429631233215}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6806844472885132}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6775718927383423}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6750664114952087}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.671985387802124}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6681711673736572}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6633097529411316}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6593683958053589}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6549338698387146}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.651195228099823}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6923438310623169}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6918922066688538}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6910483837127686}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6901845932006836}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6892306804656982}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6881222724914551}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.687284529209137}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6858338117599487}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6839168071746826}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6819285154342651}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6797075867652893}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.677314281463623}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6738855838775635}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6701549291610718}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6661548614501953}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.6616603136062622}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6567655801773071}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.6525762677192688}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.6506041884422302}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.6440920233726501}], \"data-2523b8777aef409d03e0e8ad909392c6\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.522741436958313}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5297200679779053}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5272596478462219}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5329861044883728}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.5412853360176086}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.5506224036216736}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.5734796524047852}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.5668776035308838}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.5722594261169434}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.5814061164855957}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.5908656120300293}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.591934859752655}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.598457932472229}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.6036047339439392}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.6096360683441162}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6123749017715454}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6164758205413818}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6220356822013855}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6259284019470215}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6262599229812622}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5067225098609924}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5025289058685303}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.512091338634491}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5221544504165649}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.5367293357849121}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.5524086356163025}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.541648805141449}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.5527156591415405}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.5775092244148254}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.5861095190048218}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.5897363424301147}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.5868055820465088}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6007336974143982}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6073216795921326}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.604993999004364}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.6091777086257935}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.6115360260009766}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6101800203323364}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.5976816415786743}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6310679316520691}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(lstm_stop_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the classification report, it shows that the performance without encoded categorical 'channel' is slightly worse than with. This means that all those columns should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442/442 [==============================] - 16s 35ms/step\n",
      "190/190 [==============================] - 7s 35ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64      6972\n",
      "           1       0.65      0.60      0.62      7144\n",
      "\n",
      "    accuracy                           0.63     14116\n",
      "   macro avg       0.63      0.63      0.63     14116\n",
      "weighted avg       0.63      0.63      0.63     14116\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.67      0.65      3111\n",
      "           1       0.63      0.60      0.61      2939\n",
      "\n",
      "    accuracy                           0.63      6050\n",
      "   macro avg       0.63      0.63      0.63      6050\n",
      "weighted avg       0.63      0.63      0.63      6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(lstm_stop, X_train_token, y_train, X_val_token, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the model performs worse when only being trained on text data. This means, that encoded categorical 'channel' shall be kept as feature for the final model. Next step is to train a LSTM on both text data excluding stop-words and encoded categorical 'channel'.\n",
    "\n",
    "This step is not executed in the scope of this project. It is assumed that removing stop-words does not contribute in performing significant better than without stop-words meaning the easier way of just keeping the stop-words is followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(opt, embedding=emb, categorical=cat, cat_input=categorical_input, t_input=text_input):\n",
    "\n",
    "    create_lstm = LSTM(256)(embedding)\n",
    "    lstm_comb = Concatenate()([create_lstm, categorical])\n",
    "    lstm_d1 = Dense(128, activation='relu')(lstm_comb)\n",
    "    lstm_dr1 = Dropout(0.4)(lstm_d1)\n",
    "    lstm_d2 = Dense(64, activation='relu')(lstm_dr1)\n",
    "    lstm_o = Dense(1, activation='sigmoid')(lstm_d2)\n",
    "    model = Model(inputs=[cat_input, t_input], outputs=lstm_o)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparametertuning, a self-defined function is defined which returns a LSTM based on the initial LSTM architecture.\n",
    "Ten models using the same architecture but different hyperparameters are initialized and trained on the training data. The hyperparameters are chosen at random in a pre-defined scope trying to mimic a random search algorithm. Based on the performance of those models, the best hyperparameters are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = [\"sgd\", \"adam\", \"RMSProp\"]\n",
    "b = [32, 64, 128]\n",
    "e = [10, 20, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111/111 [==============================] - 21s 176ms/step - loss: 0.5930 - precision_12: 0.6581 - val_loss: 0.5578 - val_precision_12: 0.6325\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.5450 - precision_12: 0.6881 - val_loss: 0.5215 - val_precision_12: 0.6811\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5101 - precision_12: 0.7146 - val_loss: 0.5238 - val_precision_12: 0.7142\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 18s 164ms/step - loss: 0.4863 - precision_12: 0.7280 - val_loss: 0.4968 - val_precision_12: 0.6847\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 20s 177ms/step - loss: 0.4439 - precision_12: 0.7563 - val_loss: 0.4799 - val_precision_12: 0.7184\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 19s 175ms/step - loss: 0.4072 - precision_12: 0.7746 - val_loss: 0.4719 - val_precision_12: 0.7433\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 19s 172ms/step - loss: 0.3714 - precision_12: 0.8078 - val_loss: 0.4551 - val_precision_12: 0.7486\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.3260 - precision_12: 0.8326 - val_loss: 0.4692 - val_precision_12: 0.7821\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 19s 173ms/step - loss: 0.2792 - precision_12: 0.8635 - val_loss: 0.4551 - val_precision_12: 0.7673\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.2319 - precision_12: 0.8923 - val_loss: 0.4631 - val_precision_12: 0.7738\n",
      "Epoch 1/40\n",
      "111/111 [==============================] - 20s 171ms/step - loss: 0.6909 - precision_13: 0.5277 - val_loss: 0.6826 - val_precision_13: 0.5988\n",
      "Epoch 2/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.6799 - precision_13: 0.5943 - val_loss: 0.6715 - val_precision_13: 0.6039\n",
      "Epoch 3/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.6714 - precision_13: 0.6099 - val_loss: 0.6611 - val_precision_13: 0.6050\n",
      "Epoch 4/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.6622 - precision_13: 0.6152 - val_loss: 0.6507 - val_precision_13: 0.6054\n",
      "Epoch 5/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.6527 - precision_13: 0.6192 - val_loss: 0.6403 - val_precision_13: 0.6059\n",
      "Epoch 6/40\n",
      "111/111 [==============================] - 19s 168ms/step - loss: 0.6435 - precision_13: 0.6189 - val_loss: 0.6302 - val_precision_13: 0.6070\n",
      "Epoch 7/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.6360 - precision_13: 0.6227 - val_loss: 0.6219 - val_precision_13: 0.6058\n",
      "Epoch 8/40\n",
      "111/111 [==============================] - 18s 163ms/step - loss: 0.6289 - precision_13: 0.6211 - val_loss: 0.6137 - val_precision_13: 0.6068\n",
      "Epoch 9/40\n",
      "111/111 [==============================] - 18s 166ms/step - loss: 0.6248 - precision_13: 0.6235 - val_loss: 0.6072 - val_precision_13: 0.6134\n",
      "Epoch 10/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.6179 - precision_13: 0.6264 - val_loss: 0.6020 - val_precision_13: 0.6094\n",
      "Epoch 11/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.6140 - precision_13: 0.6283 - val_loss: 0.5973 - val_precision_13: 0.6153\n",
      "Epoch 12/40\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6123 - precision_13: 0.6275 - val_loss: 0.5937 - val_precision_13: 0.6198\n",
      "Epoch 13/40\n",
      "111/111 [==============================] - 20s 180ms/step - loss: 0.6065 - precision_13: 0.6320 - val_loss: 0.5908 - val_precision_13: 0.6170\n",
      "Epoch 14/40\n",
      "111/111 [==============================] - 20s 182ms/step - loss: 0.6044 - precision_13: 0.6319 - val_loss: 0.5883 - val_precision_13: 0.6179\n",
      "Epoch 15/40\n",
      "111/111 [==============================] - 19s 167ms/step - loss: 0.6025 - precision_13: 0.6347 - val_loss: 0.5854 - val_precision_13: 0.6234\n",
      "Epoch 16/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.5996 - precision_13: 0.6408 - val_loss: 0.5830 - val_precision_13: 0.6250\n",
      "Epoch 17/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.5974 - precision_13: 0.6426 - val_loss: 0.5806 - val_precision_13: 0.6338\n",
      "Epoch 18/40\n",
      "111/111 [==============================] - 18s 166ms/step - loss: 0.5973 - precision_13: 0.6417 - val_loss: 0.5788 - val_precision_13: 0.6376\n",
      "Epoch 19/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.5934 - precision_13: 0.6503 - val_loss: 0.5786 - val_precision_13: 0.6284\n",
      "Epoch 20/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5915 - precision_13: 0.6486 - val_loss: 0.5753 - val_precision_13: 0.6389\n",
      "Epoch 21/40\n",
      "111/111 [==============================] - 19s 167ms/step - loss: 0.5903 - precision_13: 0.6509 - val_loss: 0.5743 - val_precision_13: 0.6379\n",
      "Epoch 22/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.5890 - precision_13: 0.6549 - val_loss: 0.5722 - val_precision_13: 0.6436\n",
      "Epoch 23/40\n",
      "111/111 [==============================] - 18s 166ms/step - loss: 0.5880 - precision_13: 0.6571 - val_loss: 0.5703 - val_precision_13: 0.6514\n",
      "Epoch 24/40\n",
      "111/111 [==============================] - 18s 163ms/step - loss: 0.5854 - precision_13: 0.6602 - val_loss: 0.5692 - val_precision_13: 0.6472\n",
      "Epoch 25/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5841 - precision_13: 0.6602 - val_loss: 0.5675 - val_precision_13: 0.6495\n",
      "Epoch 26/40\n",
      "111/111 [==============================] - 21s 187ms/step - loss: 0.5808 - precision_13: 0.6634 - val_loss: 0.5656 - val_precision_13: 0.6637\n",
      "Epoch 27/40\n",
      "111/111 [==============================] - 19s 173ms/step - loss: 0.5807 - precision_13: 0.6637 - val_loss: 0.5644 - val_precision_13: 0.6523\n",
      "Epoch 28/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5796 - precision_13: 0.6671 - val_loss: 0.5636 - val_precision_13: 0.6495\n",
      "Epoch 29/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.5774 - precision_13: 0.6657 - val_loss: 0.5645 - val_precision_13: 0.6442\n",
      "Epoch 30/40\n",
      "111/111 [==============================] - 19s 172ms/step - loss: 0.5758 - precision_13: 0.6701 - val_loss: 0.5641 - val_precision_13: 0.6438\n",
      "Epoch 31/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5748 - precision_13: 0.6707 - val_loss: 0.5628 - val_precision_13: 0.6950\n",
      "Epoch 32/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5724 - precision_13: 0.6763 - val_loss: 0.5701 - val_precision_13: 0.6343\n",
      "Epoch 33/40\n",
      "111/111 [==============================] - 19s 167ms/step - loss: 0.5721 - precision_13: 0.6710 - val_loss: 0.5563 - val_precision_13: 0.6746\n",
      "Epoch 34/40\n",
      "111/111 [==============================] - 19s 170ms/step - loss: 0.5701 - precision_13: 0.6775 - val_loss: 0.5598 - val_precision_13: 0.6499\n",
      "Epoch 35/40\n",
      "111/111 [==============================] - 19s 168ms/step - loss: 0.5690 - precision_13: 0.6765 - val_loss: 0.5647 - val_precision_13: 0.6389\n",
      "Epoch 36/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5683 - precision_13: 0.6756 - val_loss: 0.5546 - val_precision_13: 0.6890\n",
      "Epoch 37/40\n",
      "111/111 [==============================] - 21s 189ms/step - loss: 0.5693 - precision_13: 0.6761 - val_loss: 0.5600 - val_precision_13: 0.7054\n",
      "Epoch 38/40\n",
      "111/111 [==============================] - 19s 171ms/step - loss: 0.5665 - precision_13: 0.6772 - val_loss: 0.5692 - val_precision_13: 0.6324\n",
      "Epoch 39/40\n",
      "111/111 [==============================] - 19s 169ms/step - loss: 0.5651 - precision_13: 0.6804 - val_loss: 0.5562 - val_precision_13: 0.6527\n",
      "Epoch 40/40\n",
      "111/111 [==============================] - 19s 167ms/step - loss: 0.5649 - precision_13: 0.6813 - val_loss: 0.5514 - val_precision_13: 0.6921\n",
      "Epoch 1/10\n",
      "221/221 [==============================] - 28s 120ms/step - loss: 0.5830 - precision_14: 0.6619 - val_loss: 0.5371 - val_precision_14: 0.6605\n",
      "Epoch 2/10\n",
      "221/221 [==============================] - 25s 113ms/step - loss: 0.5417 - precision_14: 0.6918 - val_loss: 0.5233 - val_precision_14: 0.6603\n",
      "Epoch 3/10\n",
      "221/221 [==============================] - 26s 118ms/step - loss: 0.5018 - precision_14: 0.7215 - val_loss: 0.5118 - val_precision_14: 0.6716\n",
      "Epoch 4/10\n",
      "221/221 [==============================] - 26s 118ms/step - loss: 0.4655 - precision_14: 0.7494 - val_loss: 0.5434 - val_precision_14: 0.6631\n",
      "Epoch 5/10\n",
      "221/221 [==============================] - 26s 120ms/step - loss: 0.4178 - precision_14: 0.7798 - val_loss: 0.4928 - val_precision_14: 0.7306\n",
      "Epoch 6/10\n",
      "221/221 [==============================] - 27s 122ms/step - loss: 0.3723 - precision_14: 0.8108 - val_loss: 0.4691 - val_precision_14: 0.7824\n",
      "Epoch 7/10\n",
      "221/221 [==============================] - 28s 127ms/step - loss: 0.3238 - precision_14: 0.8443 - val_loss: 0.4710 - val_precision_14: 0.7660\n",
      "Epoch 8/10\n",
      "221/221 [==============================] - 26s 117ms/step - loss: 0.2797 - precision_14: 0.8705 - val_loss: 0.5254 - val_precision_14: 0.7840\n",
      "Epoch 9/10\n",
      "221/221 [==============================] - 27s 124ms/step - loss: 0.2388 - precision_14: 0.8933 - val_loss: 0.4471 - val_precision_14: 0.7978\n",
      "Epoch 10/10\n",
      "221/221 [==============================] - 25s 114ms/step - loss: 0.1985 - precision_14: 0.9162 - val_loss: 0.4898 - val_precision_14: 0.8230\n",
      "Epoch 1/10\n",
      "442/442 [==============================] - 40s 88ms/step - loss: 0.6779 - precision_15: 0.6164 - val_loss: 0.6562 - val_precision_15: 0.6034\n",
      "Epoch 2/10\n",
      "442/442 [==============================] - 41s 93ms/step - loss: 0.6476 - precision_15: 0.6220 - val_loss: 0.6207 - val_precision_15: 0.6120\n",
      "Epoch 3/10\n",
      "442/442 [==============================] - 39s 88ms/step - loss: 0.6217 - precision_15: 0.6294 - val_loss: 0.5988 - val_precision_15: 0.6149\n",
      "Epoch 4/10\n",
      "442/442 [==============================] - 40s 92ms/step - loss: 0.6101 - precision_15: 0.6317 - val_loss: 0.5952 - val_precision_15: 0.6112\n",
      "Epoch 5/10\n",
      "442/442 [==============================] - 40s 90ms/step - loss: 0.6017 - precision_15: 0.6390 - val_loss: 0.5830 - val_precision_15: 0.6260\n",
      "Epoch 6/10\n",
      "442/442 [==============================] - 40s 90ms/step - loss: 0.5939 - precision_15: 0.6485 - val_loss: 0.5785 - val_precision_15: 0.6695\n",
      "Epoch 7/10\n",
      "442/442 [==============================] - 40s 91ms/step - loss: 0.5890 - precision_15: 0.6537 - val_loss: 0.5711 - val_precision_15: 0.6401\n",
      "Epoch 8/10\n",
      "442/442 [==============================] - 40s 91ms/step - loss: 0.5819 - precision_15: 0.6614 - val_loss: 0.5756 - val_precision_15: 0.6284\n",
      "Epoch 9/10\n",
      "442/442 [==============================] - 40s 91ms/step - loss: 0.5779 - precision_15: 0.6679 - val_loss: 0.5602 - val_precision_15: 0.6510\n",
      "Epoch 10/10\n",
      "442/442 [==============================] - 40s 91ms/step - loss: 0.5733 - precision_15: 0.6729 - val_loss: 0.5642 - val_precision_15: 0.7094\n",
      "Epoch 1/10\n",
      "221/221 [==============================] - 25s 109ms/step - loss: 0.5818 - precision_16: 0.6577 - val_loss: 0.5503 - val_precision_16: 0.6371\n",
      "Epoch 2/10\n",
      "221/221 [==============================] - 25s 113ms/step - loss: 0.5371 - precision_16: 0.6916 - val_loss: 0.5240 - val_precision_16: 0.6719\n",
      "Epoch 3/10\n",
      "221/221 [==============================] - 25s 114ms/step - loss: 0.5032 - precision_16: 0.7186 - val_loss: 0.5007 - val_precision_16: 0.7100\n",
      "Epoch 4/10\n",
      "221/221 [==============================] - 26s 117ms/step - loss: 0.4617 - precision_16: 0.7459 - val_loss: 0.4935 - val_precision_16: 0.7195\n",
      "Epoch 5/10\n",
      "221/221 [==============================] - 27s 121ms/step - loss: 0.4149 - precision_16: 0.7780 - val_loss: 0.4791 - val_precision_16: 0.7243\n",
      "Epoch 6/10\n",
      "221/221 [==============================] - 25s 113ms/step - loss: 0.3669 - precision_16: 0.8102 - val_loss: 0.4926 - val_precision_16: 0.7071\n",
      "Epoch 7/10\n",
      "221/221 [==============================] - 26s 119ms/step - loss: 0.3170 - precision_16: 0.8434 - val_loss: 0.4382 - val_precision_16: 0.7859\n",
      "Epoch 8/10\n",
      "221/221 [==============================] - 26s 118ms/step - loss: 0.2568 - precision_16: 0.8816 - val_loss: 0.4586 - val_precision_16: 0.8015\n",
      "Epoch 9/10\n",
      "221/221 [==============================] - 26s 120ms/step - loss: 0.2081 - precision_16: 0.9060 - val_loss: 0.4525 - val_precision_16: 0.8168\n",
      "Epoch 10/10\n",
      "221/221 [==============================] - 26s 117ms/step - loss: 0.1583 - precision_16: 0.9334 - val_loss: 0.5646 - val_precision_16: 0.7626\n"
     ]
    }
   ],
   "source": [
    "hyper_results = list()\n",
    "models = list()\n",
    "\n",
    "for i in range(5):\n",
    "    optimizer = random.choice(opt)\n",
    "    batch = random.choice(b)\n",
    "    epochs = random.choice(e)\n",
    "\n",
    "    model = createModel(optimizer)\n",
    "    models.append(model)\n",
    "\n",
    "    history = model.fit([X_train_enc, X_train_token], y_train, batch_size=batch, epochs=epochs, validation_data=([X_val_enc, X_val_token], y_val))\n",
    "\n",
    "    hist_keys = list(history.history.keys())\n",
    "\n",
    "    key_loss = hist_keys[0]\n",
    "    key_precision = hist_keys[1]\n",
    "    key_val_loss = hist_keys[2]\n",
    "    key_val_precision = hist_keys[3]\n",
    "\n",
    "    hyper_results.append({\"config\": f\"{optimizer}, {batch}, {epochs}\", \"loss\": history.history[key_loss][-1], \"val_loss\": history.history[key_val_loss][-1], \n",
    "                          \"precision\": history.history[key_precision][-1], \"val_precision\": history.history[key_val_precision][-1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on all 5 models which were trained using different hyperparameters, the best model is chosen based on loss, val_loss, precision and val_precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>val_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adam, 128, 10</td>\n",
       "      <td>0.231942</td>\n",
       "      <td>0.463090</td>\n",
       "      <td>0.892287</td>\n",
       "      <td>0.773771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sgd, 128, 40</td>\n",
       "      <td>0.564853</td>\n",
       "      <td>0.551418</td>\n",
       "      <td>0.681331</td>\n",
       "      <td>0.692108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RMSProp, 64, 10</td>\n",
       "      <td>0.198452</td>\n",
       "      <td>0.489849</td>\n",
       "      <td>0.916231</td>\n",
       "      <td>0.823023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgd, 32, 10</td>\n",
       "      <td>0.573319</td>\n",
       "      <td>0.564204</td>\n",
       "      <td>0.672876</td>\n",
       "      <td>0.709437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adam, 64, 10</td>\n",
       "      <td>0.158289</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.933407</td>\n",
       "      <td>0.762580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config      loss  val_loss  precision  val_precision\n",
       "0    adam, 128, 10  0.231942  0.463090   0.892287       0.773771\n",
       "1     sgd, 128, 40  0.564853  0.551418   0.681331       0.692108\n",
       "2  RMSProp, 64, 10  0.198452  0.489849   0.916231       0.823023\n",
       "3      sgd, 32, 10  0.573319  0.564204   0.672876       0.709437\n",
       "4     adam, 64, 10  0.158289  0.564642   0.933407       0.762580"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_results_df = pd.DataFrame.from_dict(hyper_results)\n",
    "hyper_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the third model in previously displayed DataFrame. The model is using RMSProp as optimizer, a batch size of 64 and 10 epochs. The reason why this model is chosen is because the values of precision, loss and also the validation metrics are on an overall good level compared to the other models. The model is displayed last for e.g. has a higher precision the training data, but performs worse on the validation data indicating overfitting. That's why the model with RMSProp, a batch size of 64 and 10 epochs is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and evaluating the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the performance of the best model is evaluated using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"data/LIAR_processed.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the best performances were made achieved using the tokenized text data and the encoded categorical 'channel', the test data is prepared in the same way.\n",
    "\n",
    "Due to a mistake in [data preprocessing](04_data-preprocessing.ipynb), the encoded categorical column \"channel_TikTok\" was not created for the test data. This is due to the fact that the test data goes back to a date where TikTok was not known in the way it is known today. So, to comply the data structure of the training data, the column \"channel_TikTok\" is added to the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop([\"statement\", \"statement_stop\", \"token_stop\", \"truth\"], axis=1)\n",
    "X_test[\"channel_TikTok\"] = 0\n",
    "y_test = test[\"truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_token, X_test_enc = prepareFeatures(X_test)\n",
    "y_test = prepareTarget(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finally test the performance of the chosen and optimized model, the test data consisting of the LIAR dataset is used to determine whether the models performance on unseen data comes close to the performances on the training and validation data. The model is now trained on the full training data, using the test data as validation data and evaluating the performance using the same methods as done with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc_full = np.concatenate([X_train_enc, X_val_enc], axis=0)\n",
    "X_train_token_full = np.concatenate([X_train_token, X_val_token], axis=0)\n",
    "y_train_full = np.concatenate([y_train, y_val], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is now trained on the complete training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "316/316 [==============================] - 37s 117ms/step - loss: 0.0386 - precision_14: 0.9860 - val_loss: 2.6816 - val_precision_14: 0.7368\n",
      "Epoch 2/10\n",
      "316/316 [==============================] - 37s 118ms/step - loss: 0.0358 - precision_14: 0.9871 - val_loss: 2.1331 - val_precision_14: 0.7116\n",
      "Epoch 3/10\n",
      "316/316 [==============================] - 40s 128ms/step - loss: 0.0319 - precision_14: 0.9897 - val_loss: 2.5447 - val_precision_14: 0.7378\n",
      "Epoch 4/10\n",
      "316/316 [==============================] - 40s 126ms/step - loss: 0.0287 - precision_14: 0.9898 - val_loss: 2.6173 - val_precision_14: 0.7232\n",
      "Epoch 5/10\n",
      "316/316 [==============================] - 39s 124ms/step - loss: 0.0274 - precision_14: 0.9916 - val_loss: 2.6551 - val_precision_14: 0.7132\n",
      "Epoch 6/10\n",
      "316/316 [==============================] - 40s 125ms/step - loss: 0.0270 - precision_14: 0.9903 - val_loss: 2.5233 - val_precision_14: 0.7083\n",
      "Epoch 7/10\n",
      "316/316 [==============================] - 40s 125ms/step - loss: 0.0379 - precision_14: 0.9930 - val_loss: 2.7813 - val_precision_14: 0.7283\n",
      "Epoch 8/10\n",
      "316/316 [==============================] - 40s 125ms/step - loss: 0.0237 - precision_14: 0.9918 - val_loss: 2.5832 - val_precision_14: 0.7156\n",
      "Epoch 9/10\n",
      "316/316 [==============================] - 39s 125ms/step - loss: 0.0218 - precision_14: 0.9937 - val_loss: 2.8319 - val_precision_14: 0.7322\n",
      "Epoch 10/10\n",
      "316/316 [==============================] - 40s 125ms/step - loss: 0.0209 - precision_14: 0.9936 - val_loss: 2.9221 - val_precision_14: 0.7377\n"
     ]
    }
   ],
   "source": [
    "final_hist = final_model.fit([X_train_enc_full, X_train_token_full], y_train_full, batch_size=64, epochs=10, validation_data=([X_test_enc, X_test_token], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of the training process in combination with the performance on the test data shows clear signs of overfitting on the training data. The model performs extremely good, nearly perfect on the training data. The performance of the test data does differ significantly. The precision on the training data is close at around 99%, while the precision on the test data ranges between 70 and 74%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-70bdbf695d94416e9ed44268ab4b3a24.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-70bdbf695d94416e9ed44268ab4b3a24.vega-embed details,\n",
       "  #altair-viz-70bdbf695d94416e9ed44268ab4b3a24.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-70bdbf695d94416e9ed44268ab4b3a24\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-70bdbf695d94416e9ed44268ab4b3a24\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-70bdbf695d94416e9ed44268ab4b3a24\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-a705bee9ac01723038adacdb439c57f2\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.02093995176255703, 2.9220948219299316]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-2f6446b0b5a59146041b7ba843a300a4\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.7083272933959961, 0.9936564564704895]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-a705bee9ac01723038adacdb439c57f2\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.03857501223683357}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.035813409835100174}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.03186492994427681}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.028666701167821884}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.027440285310149193}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.027045950293540955}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.03790774568915367}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.023707907646894455}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.02182774804532528}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.02093995176255703}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 2.6815531253814697}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 2.1331253051757812}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 2.5447442531585693}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 2.6173224449157715}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 2.6550705432891846}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 2.5232698917388916}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 2.781287908554077}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 2.5832467079162598}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 2.831875801086426}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 2.9220948219299316}], \"data-2f6446b0b5a59146041b7ba843a300a4\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.9860395789146423}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.987127423286438}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.9896907210350037}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.9897857904434204}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.9915758371353149}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.9902951121330261}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.9929521679878235}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.9917707443237305}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.9936564564704895}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.9935560822486877}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.7368007898330688}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.7115634679794312}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.7377609610557556}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.7232017517089844}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.7132293581962585}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.7083272933959961}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.7283105254173279}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.7156391143798828}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.7322115302085876}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.7377104163169861}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(final_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification reports show the same signs of overfitting. As seen below, the precision and accuracy on the training data is at 100% meaning every instance of the data was predicted correctly. However, the performance on the test data does not reach the same level as the precision is around 68%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631/631 [==============================] - 20s 32ms/step\n",
      "398/398 [==============================] - 13s 33ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10083\n",
      "           1       1.00      1.00      1.00     10083\n",
      "\n",
      "    accuracy                           1.00     20166\n",
      "   macro avg       1.00      1.00      1.00     20166\n",
      "weighted avg       1.00      1.00      1.00     20166\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.72      0.65      5611\n",
      "           1       0.74      0.62      0.67      7100\n",
      "\n",
      "    accuracy                           0.66     12711\n",
      "   macro avg       0.67      0.67      0.66     12711\n",
      "weighted avg       0.68      0.66      0.66     12711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(final_model, [X_train_enc_full, X_train_token_full], y_train_full, [X_test_enc, X_test_token], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data can now be used for evaluating the performance of the final model. All instances are predicted by the model. As the model outputs a probability for the instance to belong to class 1 (\"true\"), all values bigger than 0.5 are assigned to 1. Values below 0.5 are assigned to class 0 (\"false\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 14s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predict = (final_model.predict([X_test_enc, X_test_token]) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a confusion matrix is plotted to further visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=[\"False\", \"True\"], yticklabels=[\"False\", \"True\"])\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the confusion matrix shows solid, but not good performance of the final model on the test data. The model predicts more instances of the data to be false while the majority of the data is actually true.\n",
    "One point which stands out is that the False Positives could be minimized. This results is exactly what was wanted to achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
