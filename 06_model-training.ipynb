{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Fake News Detection\n",
    "\n",
    "By Felix Daubner - Hochschule der Medien\n",
    "\n",
    "Module 'Supervised and Unsupervised Learning' - Prof. Dr.-Ing. Johannes Maucher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, Flatten, MaxPooling1D, Dropout, Bidirectional, Input, Concatenate\n",
    "\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "NUM_WORDS=3000\n",
    "MAX_SEQUENCE_LEN = 57\n",
    "NUM_CAT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareFeatures(X):\n",
    "    '''\n",
    "    This function gets the features and modifies the features in a way to be able to train neural network\n",
    "    using encoded categorical data and tokenized text.\n",
    "    Returns numpy arrays.\n",
    "    '''\n",
    "    X_token = np.array(X[\"token\"].apply(np.asarray))\n",
    "    X_token = np.array([arr for arr in X_token])\n",
    "\n",
    "    X_enc = np.array(X.drop([\"token\"], axis=1).apply(np.array))\n",
    "\n",
    "    return X_token, X_enc\n",
    "\n",
    "def prepareTarget(y):\n",
    "    '''\n",
    "    This function returns the target data as a numpy array.\n",
    "    '''\n",
    "    return np.array(y)\n",
    "\n",
    "def visualizeHistory(history):\n",
    "    '''\n",
    "    This function gets keras.History object and plots loss, validation loss, precision and validation precision.\n",
    "    Returns altair hconcat object containing the charts.\n",
    "    '''\n",
    "\n",
    "    l, p, v_l, v_p = history.history.keys()\n",
    "\n",
    "    data = pd.DataFrame({\"epoch\": history.epoch,\n",
    "            \"loss\": history.history[l],\n",
    "            \"val_loss\": history.history[v_l],\n",
    "            \"precision\": history.history[p],\n",
    "            \"val_precision\": history.history[v_p]})\n",
    "    \n",
    "    loss_min = min(data[\"loss\"].min(), data[\"val_loss\"].min())\n",
    "    loss_max = max(data[\"loss\"].max(), data[\"val_loss\"].max())\n",
    "\n",
    "    precision_min = min(data[\"precision\"].min(), data[\"val_precision\"].min())\n",
    "    precision_max = max(data[\"precision\"].max(), data[\"val_precision\"].max())\n",
    "\n",
    "    data_melted = data.melt('epoch', value_vars=['loss', 'val_loss', 'precision', 'val_precision'], var_name='type', value_name='value')\n",
    "    \n",
    "    data_loss = data_melted[data_melted[\"type\"].isin([\"loss\", \"val_loss\"])]\n",
    "    loss = alt.Chart(data_loss).mark_line().encode(\n",
    "        x = \"epoch\",\n",
    "        y = alt.Y(\"value\", scale = alt.Scale(domain=[loss_min, loss_max])),\n",
    "        color = alt.Color(\"type\", legend=alt.Legend(orient=\"right\"))\n",
    "    ).properties(\n",
    "        title = \"Training and Validation Loss over epochs\"\n",
    "    )\n",
    "\n",
    "    data_precision = data_melted[data_melted[\"type\"].isin([\"precision\", \"val_precision\"])]\n",
    "    precision = alt.Chart(data_precision).mark_line().encode(\n",
    "        x = \"epoch\",\n",
    "        y = alt.Y(\"value\", scale = alt.Scale(domain=[precision_min, precision_max])),\n",
    "        color = alt.Color(\"type\", legend=alt.Legend(orient=\"right\"))\n",
    "    ).properties(\n",
    "        title = \"Training and Validation Precision over epochs\"\n",
    "    )\n",
    "\n",
    "    return alt.hconcat(loss, precision).resolve_scale(color=\"independent\")\n",
    "\n",
    "\n",
    "def performanceReport(model, X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    This function gets a model, training and validation data.\n",
    "    It predicts training and validation data, compares it to the true data and prints out classification report for both, training and validation.\n",
    "    '''\n",
    "    y_pred_train = (model.predict(X_train) > 0.5).astype(int)\n",
    "    y_pred_val = (model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "    print(\"\\nClassifcation Report of Performance on Training data\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"* \"*10)\n",
    "\n",
    "    print(\"\\nClassifcation Report of Performance on Validation data\")\n",
    "    print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the model training. Different types of models should be trained and then compared to find out which model fits the challenge, to determine whether a political statement was fake-news or true, best. There are four types of models to be compared: a feedforward neural network, a Long Short-Term Memory, a bidirectional Long Short-Term Memory and Convolutional Neural Network. Those models will vary in terms of layers and hyperparameters still trying to keep them rather simple. All models are trained using the encoded categorical data of 'channel' and the tokenized statements including stop words. All models are then trained using 20 epochs and a batch size of 128. \n",
    "\n",
    "The best model is evaluated based on training and validation performance. At the end, the best two models are chosen and will be optimized in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the training and validation data is the same for every model, the preparation of the preprocessed data resulting in a structure able to train different kinds of neural networks, is only needed to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data/processed.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['statement', 'channel_Instagram', 'channel_Other', 'channel_TV',\n",
       "       'channel_TikTok', 'channel_X', 'channel_ad', 'channel_article',\n",
       "       'channel_blog', 'channel_campaign', 'channel_debate',\n",
       "       'channel_interview', 'channel_lecture', 'channel_mail',\n",
       "       'channel_podcast', 'channel_presentation', 'channel_press',\n",
       "       'channel_social media', 'channel_speech', 'channel_talk',\n",
       "       'channel_video', 'truth', 'token', 'statement_stop', 'token_stop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting defining the different models, the data is prepared for the training process. The neural network to be trained only takes numpy arrays as input. Thus, the data currently saved as a pandas DataFrame is converted in to a numpy array. In this conversion process, only \"token\", the encoded channel columns and \"truth\" are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"statement\", \"statement_stop\", \"token_stop\", \"truth\"], axis=1)\n",
    "y = data[\"truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data into features and target, the features still have to preprared for training by splitting the encoded categorical data from the tokenized and padded statements. The statement data has to be taken care of using an Embedding Layer while a Dense layer is sufficient to handle the encoded categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token, X_train_enc = prepareFeatures(X_train)\n",
    "X_val_token, X_val_enc = prepareFeatures(X_val)\n",
    "y_train = prepareTarget(y_train)\n",
    "y_val = prepareTarget(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, the data despite being prepared to fit the structure of neural networks, is not ready for training yet. The tokenized statements saved in \"X_train_token\" and \"X_val_token\" need to be transformed into a embedding matrix which assigns every word / token a vector. This vector represents the word in a multi-dimensional vector space and models the relationship between different words.\n",
    "\n",
    "A pre-trained word embedding from FastText which already contains the vectors for each word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to remember the words which are placed behind each token, the trained 'tokenizer'-object of section [data pre-processing](03_data-understanding.ipynb) is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer/tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates an embedding matrix which assigns every word their respective vector as saved in the pre-trained word embedding of FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300  \n",
    "word_index = tokenizer.word_index \n",
    "num_words = min(len(word_index) + 1, NUM_WORDS)  \n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        if word in word2vec.key_to_index:\n",
    "            embedding_vector = word2vec[word]\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every kind of model which is going to be trained in the following, two input layers are defined. Those two input layers are the same used for every kind of model as the input data doesn't vary between types of models.\n",
    "\n",
    "There is one input layer for tokenized text data and another input layer for encoded categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(MAX_SEQUENCE_LEN,), name=\"text_input\")\n",
    "categorical_input = Input(shape=(NUM_CAT,), name=\"categorical_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input layer and embedding matrix, an Embedding layer can be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(NUM_WORDS, embedding_dim, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LEN, trainable=False)(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the encoded categorical data, a dense layer is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Dense(32, activation=\"relu\")(categorical_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at fake-news detection, it is decided which metric should be optimized. Classification provide a lots of useful metrics which have to be chosen for each project individually. The most common and known metrics are accuracy, precision, recall and f1-score.\n",
    "\n",
    "Most often, accuracy is not a good metric as it doesn't take into account the cost of predicting errors. That's why either precision or recall should be used.\n",
    "\n",
    "The worst case at fake-news is when a fake-news statement is not identified as fake-news but as true. Whereas the other way, a true statement being classified as fake-news statement does not harm in the same way. Translating this into the terms of this project means a false positive (\"a statement which is 'fake' (0) gets classified as 'true' (1)\") is worse than a false negative (\"a statement which is 'true' (1) gets classified as 'false' (0)\"). The metrics focusing on optimizing the false positives is precision. Therefore, precision is used when trying to chose and optimize a fake-news classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, four different types models are trained and evaluated. Based on those evaluations, the best model is chosen. \n",
    "The next section [optimization](07_evaluation-optimization.ipynb) handles feature extraction and hyperparameter tuning of the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Nerwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model to be trained is a simple feedforward neural network. The feedforward neural network consists of Dense and Dropout layers which make the architecture quite easy and not too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_flatten_text = Flatten()(emb)\n",
    "\n",
    "ff_combined = Concatenate()([ff_flatten_text, cat])\n",
    "ff_dense1 = Dense(128, activation=\"relu\")(ff_combined)\n",
    "ff_drop = Dropout(0.4)(ff_dense1)\n",
    "ff_dense2 = Dense(32, activation=\"relu\")(ff_drop)\n",
    "ff_output = Dense(1, activation=\"sigmoid\")(ff_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 17100)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 17132)        0           ['flatten[0][0]',                \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          2193024     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           4128        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            33          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,097,857\n",
      "Trainable params: 2,197,857\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff = Model(inputs=[categorical_input, text_input], outputs=ff_output)\n",
    "ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 16:29:46.467188: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 11ms/step - loss: 0.6908 - precision: 0.5336 - val_loss: 0.6873 - val_precision: 0.5234\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.6838 - precision: 0.5669 - val_loss: 0.6808 - val_precision: 0.5679\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.6751 - precision: 0.5921 - val_loss: 0.6725 - val_precision: 0.5760\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.6687 - precision: 0.6075 - val_loss: 0.6659 - val_precision: 0.5827\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 0.6611 - precision: 0.6175 - val_loss: 0.6582 - val_precision: 0.5949\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.6540 - precision: 0.6263 - val_loss: 0.6501 - val_precision: 0.6075\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.6452 - precision: 0.6408 - val_loss: 0.6418 - val_precision: 0.6150\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.6368 - precision: 0.6521 - val_loss: 0.6317 - val_precision: 0.6352\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.6268 - precision: 0.6675 - val_loss: 0.6238 - val_precision: 0.6341\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.6168 - precision: 0.6827 - val_loss: 0.6125 - val_precision: 0.6602\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.6067 - precision: 0.6922 - val_loss: 0.6041 - val_precision: 0.6603\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.5955 - precision: 0.6989 - val_loss: 0.5965 - val_precision: 0.6610\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.5856 - precision: 0.7058 - val_loss: 0.5828 - val_precision: 0.7040\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.5752 - precision: 0.7121 - val_loss: 0.5747 - val_precision: 0.7006\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.5651 - precision: 0.7179 - val_loss: 0.5669 - val_precision: 0.6985\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.5545 - precision: 0.7260 - val_loss: 0.5580 - val_precision: 0.7040\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.5414 - precision: 0.7351 - val_loss: 0.5515 - val_precision: 0.7036\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.5318 - precision: 0.7451 - val_loss: 0.5459 - val_precision: 0.6947\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.5211 - precision: 0.7488 - val_loss: 0.5351 - val_precision: 0.7253\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 0.5112 - precision: 0.7554 - val_loss: 0.5285 - val_precision: 0.7248\n"
     ]
    }
   ],
   "source": [
    "ff_hist = ff.fit([X_train_enc, X_train_token], y_train, epochs=20, batch_size=128, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the feedforward neural network, there are some things standing out.\n",
    "\n",
    "As seen in the visualizatons below, both the training and validation loss decline per epoch. Same goes for precision which inclines per epoch. Both metrics show signs of overfitting as the training metrics perform significantly better compared to the validation metrics. Overfitting should be avoided as the performance on new, unseen data is significantly worse than the performance on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9926f0ec181f40cfacf84dc59d355cd3.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9926f0ec181f40cfacf84dc59d355cd3.vega-embed details,\n",
       "  #altair-viz-9926f0ec181f40cfacf84dc59d355cd3.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9926f0ec181f40cfacf84dc59d355cd3\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9926f0ec181f40cfacf84dc59d355cd3\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9926f0ec181f40cfacf84dc59d355cd3\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-19e594b004225deba27e6db59c21ffa7\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5112316012382507, 0.6907914280891418]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-b5f3b1ba80c2db8772fe6783104b85c8\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.523366391658783, 0.755407452583313]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-19e594b004225deba27e6db59c21ffa7\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6907914280891418}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6837975978851318}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6751409769058228}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6686716079711914}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6611156463623047}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6540411114692688}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6452486515045166}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6367782950401306}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6268188953399658}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6168451309204102}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6067233085632324}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.5954505801200867}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.5855876803398132}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.5751809477806091}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.5651271939277649}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.5544717311859131}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.5413796305656433}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.5318175554275513}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.5211334824562073}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.5112316012382507}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6872623562812805}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6808232069015503}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6725091338157654}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6659034490585327}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6582475900650024}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6501175761222839}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.641785740852356}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6317433714866638}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6238004565238953}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6125336289405823}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6040751934051514}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.5965461134910583}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.582821786403656}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.5747280120849609}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.5669399499893188}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.5579824447631836}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.5514854788780212}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.5459015965461731}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.5350624322891235}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.528484046459198}], \"data-b5f3b1ba80c2db8772fe6783104b85c8\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5335887670516968}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5669199824333191}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5920974016189575}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.6074728965759277}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.6174812316894531}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.6263039112091064}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.6408479809761047}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6521015167236328}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6674658060073853}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6826909780502319}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.6921688318252563}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6989277005195618}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.7057681679725647}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.7121402025222778}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.7178709506988525}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.7260255217552185}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.7351146340370178}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.7451493144035339}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.7488196492195129}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.755407452583313}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.523366391658783}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5678615570068359}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5760208368301392}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5827465057373047}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.5949008464813232}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.6075149774551392}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.614987850189209}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.6352261900901794}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.6340968608856201}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.6602091193199158}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.6602985262870789}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.6609686613082886}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.7040078639984131}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.7006348967552185}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.6984909176826477}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.7040024995803833}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.7035573124885559}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6946520805358887}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.7252817749977112}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.7247939109802246}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(ff_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the results of the classification report, the signs of overfitting still remain. Although the model performs already quite good with a weighted precision on the training data of 0.89, it definitely needs to be optimized as the weighted precision on the validation data is at only 0.81. Due to the simple architecture of the model, the feedforward neural networks is considered when evaluating the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 1s 1ms/step\n",
      "189/189 [==============================] - 0s 1ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78      7000\n",
      "           1       0.77      0.81      0.79      7109\n",
      "\n",
      "    accuracy                           0.78     14109\n",
      "   macro avg       0.79      0.78      0.78     14109\n",
      "weighted avg       0.79      0.78      0.78     14109\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74      3078\n",
      "           1       0.72      0.77      0.75      2969\n",
      "\n",
      "    accuracy                           0.74      6047\n",
      "   macro avg       0.74      0.74      0.74      6047\n",
      "weighted avg       0.74      0.74      0.74      6047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(ff, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next type is a Long Short-Term Memory Neural Network. This type of neural network ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ = LSTM(128)(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_combined = Concatenate()([lstm_, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dense1 = Dense(128, activation='relu')(lstm_combined)\n",
    "lstm_drop1 = Dropout(0.4)(lstm_dense1)\n",
    "lstm_dense2 = Dense(64, activation='relu')(lstm_drop1)\n",
    "lstm_drop2 = Dropout(0.4)(lstm_dense2)\n",
    "lstm_output = Dense(1, activation='sigmoid')(lstm_drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 64)           93440       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 96)           0           ['lstm[0][0]',                   \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          12416       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            65          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,014,849\n",
      "Trainable params: 114,849\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = Model(inputs=[categorical_input, text_input], outputs=lstm_output)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 7s 51ms/step - loss: 0.6930 - precision_1: 0.5173 - val_loss: 0.6903 - val_precision_1: 0.5464\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 6s 52ms/step - loss: 0.6894 - precision_1: 0.5339 - val_loss: 0.6874 - val_precision_1: 0.5603\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 5s 49ms/step - loss: 0.6876 - precision_1: 0.5499 - val_loss: 0.6848 - val_precision_1: 0.5749\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 5s 49ms/step - loss: 0.6847 - precision_1: 0.5612 - val_loss: 0.6818 - val_precision_1: 0.5984\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 5s 49ms/step - loss: 0.6817 - precision_1: 0.5730 - val_loss: 0.6787 - val_precision_1: 0.6042\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 6s 50ms/step - loss: 0.6795 - precision_1: 0.5883 - val_loss: 0.6754 - val_precision_1: 0.6036\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 6s 51ms/step - loss: 0.6765 - precision_1: 0.5950 - val_loss: 0.6718 - val_precision_1: 0.6045\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 6s 52ms/step - loss: 0.6734 - precision_1: 0.6026 - val_loss: 0.6678 - val_precision_1: 0.6060\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 6s 50ms/step - loss: 0.6700 - precision_1: 0.6101 - val_loss: 0.6635 - val_precision_1: 0.6055\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 5s 48ms/step - loss: 0.6657 - precision_1: 0.6137 - val_loss: 0.6589 - val_precision_1: 0.6067\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 5s 49ms/step - loss: 0.6615 - precision_1: 0.6195 - val_loss: 0.6543 - val_precision_1: 0.6072\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 5s 49ms/step - loss: 0.6577 - precision_1: 0.6267 - val_loss: 0.6499 - val_precision_1: 0.6049\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 6s 51ms/step - loss: 0.6538 - precision_1: 0.6214 - val_loss: 0.6450 - val_precision_1: 0.6102\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 6s 50ms/step - loss: 0.6523 - precision_1: 0.6211 - val_loss: 0.6407 - val_precision_1: 0.6091\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 6s 50ms/step - loss: 0.6476 - precision_1: 0.6234 - val_loss: 0.6365 - val_precision_1: 0.6070\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 6s 53ms/step - loss: 0.6417 - precision_1: 0.6293 - val_loss: 0.6318 - val_precision_1: 0.6102\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 6s 54ms/step - loss: 0.6378 - precision_1: 0.6321 - val_loss: 0.6276 - val_precision_1: 0.6177\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 6s 53ms/step - loss: 0.6362 - precision_1: 0.6293 - val_loss: 0.6238 - val_precision_1: 0.6206\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 7s 59ms/step - loss: 0.6318 - precision_1: 0.6312 - val_loss: 0.6203 - val_precision_1: 0.6212\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 6s 55ms/step - loss: 0.6281 - precision_1: 0.6355 - val_loss: 0.6169 - val_precision_1: 0.6220\n"
     ]
    }
   ],
   "source": [
    "lstm_hist = lstm.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-09f81377dfe646d99ff51fdb091efc7c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-09f81377dfe646d99ff51fdb091efc7c.vega-embed details,\n",
       "  #altair-viz-09f81377dfe646d99ff51fdb091efc7c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-09f81377dfe646d99ff51fdb091efc7c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-09f81377dfe646d99ff51fdb091efc7c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-09f81377dfe646d99ff51fdb091efc7c\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-28961075b792248d32606b0bd4890e79\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.6169360876083374, 0.6929820775985718]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-2a62fe9d4122f66b3beb07af49b1f402\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.517261803150177, 0.6355039477348328]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-28961075b792248d32606b0bd4890e79\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6929820775985718}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6893853545188904}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6875635981559753}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6846646666526794}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6817092895507812}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6794692873954773}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6765115857124329}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6733694076538086}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6700222492218018}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6657225489616394}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6615397930145264}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6577064394950867}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6538037061691284}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6523029804229736}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.6475539803504944}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6417331695556641}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6377902626991272}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6361830830574036}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6317822337150574}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.6281200051307678}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6902600526809692}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6874141693115234}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6847707033157349}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6818452477455139}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6787326335906982}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6754464507102966}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.6718239784240723}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6677907109260559}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6634927988052368}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6588935852050781}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6542887091636658}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6499176025390625}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6449866890907288}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6407178044319153}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6365370154380798}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.631828248500824}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6275872588157654}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.6238490343093872}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.6202841997146606}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.6169360876083374}], \"data-2a62fe9d4122f66b3beb07af49b1f402\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.517261803150177}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5338611602783203}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5498928427696228}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5612325072288513}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.5730350017547607}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.58832848072052}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.5949549674987793}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6026264429092407}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6101056933403015}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6136502623558044}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.6195199489593506}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6267420053482056}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.621422529220581}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.6211241483688354}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.6233781576156616}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6292846202850342}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6320934891700745}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6292989253997803}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.631154477596283}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6355039477348328}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5463730692863464}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5602881908416748}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.574877142906189}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.598446786403656}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.6041911840438843}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.6036056876182556}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.604514479637146}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.6060320734977722}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.6054742932319641}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.6067362427711487}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.6071935892105103}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.6048893928527832}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6101654171943665}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6091490983963013}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.6069827675819397}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.6101858615875244}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.6177423596382141}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6206289529800415}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.621200680732727}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6219949126243591}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(lstm_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 5s 10ms/step\n",
      "189/189 [==============================] - 2s 10ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.51      0.60      7000\n",
      "           1       0.63      0.83      0.72      7109\n",
      "\n",
      "    accuracy                           0.67     14109\n",
      "   macro avg       0.69      0.67      0.66     14109\n",
      "weighted avg       0.69      0.67      0.66     14109\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.52      0.61      3078\n",
      "           1       0.62      0.82      0.71      2969\n",
      "\n",
      "    accuracy                           0.67      6047\n",
      "   macro avg       0.69      0.67      0.66      6047\n",
      "weighted avg       0.69      0.67      0.66      6047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(lstm, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_ = Bidirectional(LSTM(128))(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_combined = Concatenate()([blstm_, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_dense1 = Dense(128, activation='relu')(blstm_combined)\n",
    "blstm_drop1 = Dropout(0.4)(blstm_dense1)\n",
    "blstm_dense2 = Dense(32, activation='relu')(blstm_drop1)\n",
    "blstm_drop2 = Dropout(0.4)(blstm_dense2)\n",
    "blstm_output = Dense(1, activation='sigmoid')(blstm_drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 256)          439296      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           672         ['categorical_input[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 288)          0           ['bidirectional[0][0]',          \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          36992       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           4128        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 32)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,381,121\n",
      "Trainable params: 481,121\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm = Model(inputs=[categorical_input, text_input], outputs=blstm_output)\n",
    "blstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 16s 135ms/step - loss: 0.6891 - precision_2: 0.5155 - val_loss: 0.6857 - val_precision_2: 0.5676\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 15s 132ms/step - loss: 0.6838 - precision_2: 0.5600 - val_loss: 0.6797 - val_precision_2: 0.6021\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 14s 130ms/step - loss: 0.6795 - precision_2: 0.5818 - val_loss: 0.6744 - val_precision_2: 0.6092\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 14s 123ms/step - loss: 0.6742 - precision_2: 0.5995 - val_loss: 0.6695 - val_precision_2: 0.6111\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 14s 127ms/step - loss: 0.6699 - precision_2: 0.6037 - val_loss: 0.6650 - val_precision_2: 0.6119\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 14s 124ms/step - loss: 0.6663 - precision_2: 0.6122 - val_loss: 0.6605 - val_precision_2: 0.6131\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 14s 129ms/step - loss: 0.6646 - precision_2: 0.6139 - val_loss: 0.6564 - val_precision_2: 0.6128\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 14s 124ms/step - loss: 0.6587 - precision_2: 0.6183 - val_loss: 0.6523 - val_precision_2: 0.6077\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 15s 132ms/step - loss: 0.6550 - precision_2: 0.6200 - val_loss: 0.6479 - val_precision_2: 0.6118\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 14s 126ms/step - loss: 0.6517 - precision_2: 0.6207 - val_loss: 0.6438 - val_precision_2: 0.6110\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6486 - precision_2: 0.6269 - val_loss: 0.6400 - val_precision_2: 0.6197\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 15s 131ms/step - loss: 0.6461 - precision_2: 0.6259 - val_loss: 0.6369 - val_precision_2: 0.6095\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 14s 130ms/step - loss: 0.6432 - precision_2: 0.6285 - val_loss: 0.6332 - val_precision_2: 0.6211\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 15s 132ms/step - loss: 0.6388 - precision_2: 0.6328 - val_loss: 0.6299 - val_precision_2: 0.6187\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 14s 130ms/step - loss: 0.6381 - precision_2: 0.6287 - val_loss: 0.6271 - val_precision_2: 0.6201\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 14s 130ms/step - loss: 0.6333 - precision_2: 0.6310 - val_loss: 0.6244 - val_precision_2: 0.6095\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 14s 127ms/step - loss: 0.6345 - precision_2: 0.6276 - val_loss: 0.6216 - val_precision_2: 0.6222\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 14s 128ms/step - loss: 0.6308 - precision_2: 0.6311 - val_loss: 0.6193 - val_precision_2: 0.6188\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 14s 129ms/step - loss: 0.6273 - precision_2: 0.6343 - val_loss: 0.6172 - val_precision_2: 0.6148\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 15s 133ms/step - loss: 0.6259 - precision_2: 0.6328 - val_loss: 0.6144 - val_precision_2: 0.6276\n"
     ]
    }
   ],
   "source": [
    "blstm_hist = blstm.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b.vega-embed details,\n",
       "  #altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ce9b4f2ef85b44b2a72c184e3cae650b\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-7f47bc8c444af8f13067d8fca978773e\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.6143732070922852, 0.6891233325004578]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-ce33b3d8fe69f4319f6d6126042a1f7b\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.5155012607574463, 0.6343274712562561]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-7f47bc8c444af8f13067d8fca978773e\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6891233325004578}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6838453412055969}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6795101761817932}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6741536259651184}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6698535680770874}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6663252115249634}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6646378636360168}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6587154865264893}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6550260782241821}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6516655087471008}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6486256122589111}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.6461178064346313}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.64324551820755}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.6387677788734436}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.6380718350410461}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6333010792732239}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6344801783561707}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6308339238166809}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6273101568222046}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.6258948445320129}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6856928467750549}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.6796619296073914}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6744396686553955}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6695039868354797}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6649978160858154}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6604639887809753}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.656358003616333}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.652265727519989}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6479416489601135}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6438357830047607}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6400490403175354}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6368592381477356}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6331918239593506}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6298707723617554}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6270963549613953}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.6244005560874939}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6215943098068237}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.6192764043807983}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.6171526312828064}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.6143732070922852}], \"data-ce33b3d8fe69f4319f6d6126042a1f7b\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.5155012607574463}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5600085258483887}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5818030834197998}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5995151400566101}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.6036592721939087}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.612162172794342}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.6138710975646973}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.6182547211647034}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.6199689507484436}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.6207361817359924}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.6268638968467712}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.6259063482284546}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.6284664869308472}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.6328350901603699}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.6287317872047424}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6310287714004517}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6275635957717896}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6310702562332153}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6343274712562561}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6328351497650146}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5675787925720215}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.6021453142166138}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.6091867685317993}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.6111252307891846}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.6119328141212463}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.6131088733673096}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.6128085851669312}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.607672393321991}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.611835777759552}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.610990047454834}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.6197364926338196}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.6094986796379089}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.6211340427398682}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.6186633110046387}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.6201372742652893}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.609510064125061}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.622188150882721}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.6188094019889832}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.6148039698600769}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6276316046714783}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(blstm_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 10s 21ms/step\n",
      "189/189 [==============================] - 4s 20ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62      7000\n",
      "           1       0.64      0.81      0.72      7109\n",
      "\n",
      "    accuracy                           0.68     14109\n",
      "   macro avg       0.69      0.68      0.67     14109\n",
      "weighted avg       0.69      0.68      0.67     14109\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62      3078\n",
      "           1       0.63      0.80      0.70      2969\n",
      "\n",
      "    accuracy                           0.67      6047\n",
      "   macro avg       0.68      0.67      0.66      6047\n",
      "weighted avg       0.68      0.67      0.66      6047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(blstm, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_ = Conv1D(filters=128, kernel_size=5, activation='relu')(emb)\n",
    "cnn_maxpool = MaxPooling1D(pool_size=5)(cnn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_flatten_text = Flatten()(cnn_)\n",
    "\n",
    "cnn_combined = Concatenate()([cnn_flatten_text, cat])\n",
    "cnn_flatten = Flatten()(cnn_maxpool)\n",
    "cnn_dense1 = Dense(128, activation=\"relu\")(cnn_flatten)\n",
    "cnn_drop = Dropout(0.4)(cnn_dense1)\n",
    "cnn_dense2 = Dense(32, activation=\"relu\")(cnn_flatten)\n",
    "cnn_drop2 = Dropout(0.4)(cnn_dense2)\n",
    "cnn_output = Dense(1, activation=\"sigmoid\")(cnn_drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      900000      ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 53, 128)      192128      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 10, 128)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1280)         0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 32)           40992       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " categorical_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            33          ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,133,153\n",
      "Trainable params: 233,153\n",
      "Non-trainable params: 900,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Model(inputs=[categorical_input, text_input], outputs=cnn_output)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6934 - precision_3: 0.4937 - val_loss: 0.6910 - val_precision_3: 0.5020\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6885 - precision_3: 0.5240 - val_loss: 0.6877 - val_precision_3: 0.5387\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6842 - precision_3: 0.5571 - val_loss: 0.6849 - val_precision_3: 0.5568\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6817 - precision_3: 0.5770 - val_loss: 0.6827 - val_precision_3: 0.5634\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6814 - precision_3: 0.5759 - val_loss: 0.6812 - val_precision_3: 0.5649\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6790 - precision_3: 0.5890 - val_loss: 0.6800 - val_precision_3: 0.5646\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6781 - precision_3: 0.5839 - val_loss: 0.6787 - val_precision_3: 0.5652\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6763 - precision_3: 0.5880 - val_loss: 0.6775 - val_precision_3: 0.5674\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 3s 23ms/step - loss: 0.6760 - precision_3: 0.5877 - val_loss: 0.6766 - val_precision_3: 0.5670\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6738 - precision_3: 0.5905 - val_loss: 0.6751 - val_precision_3: 0.5679\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6734 - precision_3: 0.5910 - val_loss: 0.6737 - val_precision_3: 0.5723\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6700 - precision_3: 0.5983 - val_loss: 0.6723 - val_precision_3: 0.5727\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6696 - precision_3: 0.5972 - val_loss: 0.6711 - val_precision_3: 0.5713\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6679 - precision_3: 0.6000 - val_loss: 0.6698 - val_precision_3: 0.5732\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6677 - precision_3: 0.5983 - val_loss: 0.6677 - val_precision_3: 0.5799\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6660 - precision_3: 0.6053 - val_loss: 0.6659 - val_precision_3: 0.5834\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 3s 24ms/step - loss: 0.6630 - precision_3: 0.6101 - val_loss: 0.6644 - val_precision_3: 0.5793\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 0.6610 - precision_3: 0.6092 - val_loss: 0.6620 - val_precision_3: 0.5851\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6603 - precision_3: 0.6138 - val_loss: 0.6597 - val_precision_3: 0.5923\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 0.6579 - precision_3: 0.6147 - val_loss: 0.6573 - val_precision_3: 0.6016\n"
     ]
    }
   ],
   "source": [
    "cnn_hist = cnn.fit([X_train_enc, X_train_token], y_train, batch_size=128, epochs=20, validation_data=([X_val_enc, X_val_token], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-47ac3b8abed0405287bec0cdf0b250c8.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-47ac3b8abed0405287bec0cdf0b250c8.vega-embed details,\n",
       "  #altair-viz-47ac3b8abed0405287bec0cdf0b250c8.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-47ac3b8abed0405287bec0cdf0b250c8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-47ac3b8abed0405287bec0cdf0b250c8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-47ac3b8abed0405287bec0cdf0b250c8\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-01353546575f93b327905282b41f0c09\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.6572684049606323, 0.6934235095977783]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Loss over epochs\"}, {\"data\": {\"name\": \"data-3dd7fde33f5bb77aa17b186747c5d1e2\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"orient\": \"right\"}, \"type\": \"nominal\"}, \"x\": {\"field\": \"epoch\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0.4936586320400238, 0.6147095561027527]}, \"type\": \"quantitative\"}}, \"title\": \"Training and Validation Precision over epochs\"}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-01353546575f93b327905282b41f0c09\": [{\"epoch\": 0, \"type\": \"loss\", \"value\": 0.6934235095977783}, {\"epoch\": 1, \"type\": \"loss\", \"value\": 0.6885282397270203}, {\"epoch\": 2, \"type\": \"loss\", \"value\": 0.6842465400695801}, {\"epoch\": 3, \"type\": \"loss\", \"value\": 0.6816583275794983}, {\"epoch\": 4, \"type\": \"loss\", \"value\": 0.6813609004020691}, {\"epoch\": 5, \"type\": \"loss\", \"value\": 0.6790249943733215}, {\"epoch\": 6, \"type\": \"loss\", \"value\": 0.6781442165374756}, {\"epoch\": 7, \"type\": \"loss\", \"value\": 0.6762521862983704}, {\"epoch\": 8, \"type\": \"loss\", \"value\": 0.6759501099586487}, {\"epoch\": 9, \"type\": \"loss\", \"value\": 0.6737505197525024}, {\"epoch\": 10, \"type\": \"loss\", \"value\": 0.6734198927879333}, {\"epoch\": 11, \"type\": \"loss\", \"value\": 0.670028805732727}, {\"epoch\": 12, \"type\": \"loss\", \"value\": 0.6696200370788574}, {\"epoch\": 13, \"type\": \"loss\", \"value\": 0.667857825756073}, {\"epoch\": 14, \"type\": \"loss\", \"value\": 0.6676652431488037}, {\"epoch\": 15, \"type\": \"loss\", \"value\": 0.6659667491912842}, {\"epoch\": 16, \"type\": \"loss\", \"value\": 0.6630172729492188}, {\"epoch\": 17, \"type\": \"loss\", \"value\": 0.6609867215156555}, {\"epoch\": 18, \"type\": \"loss\", \"value\": 0.6602962017059326}, {\"epoch\": 19, \"type\": \"loss\", \"value\": 0.6579160690307617}, {\"epoch\": 0, \"type\": \"val_loss\", \"value\": 0.6909840106964111}, {\"epoch\": 1, \"type\": \"val_loss\", \"value\": 0.687661349773407}, {\"epoch\": 2, \"type\": \"val_loss\", \"value\": 0.6849359273910522}, {\"epoch\": 3, \"type\": \"val_loss\", \"value\": 0.6827415227890015}, {\"epoch\": 4, \"type\": \"val_loss\", \"value\": 0.6812242865562439}, {\"epoch\": 5, \"type\": \"val_loss\", \"value\": 0.6800342202186584}, {\"epoch\": 6, \"type\": \"val_loss\", \"value\": 0.6786767244338989}, {\"epoch\": 7, \"type\": \"val_loss\", \"value\": 0.6775078773498535}, {\"epoch\": 8, \"type\": \"val_loss\", \"value\": 0.6765719652175903}, {\"epoch\": 9, \"type\": \"val_loss\", \"value\": 0.6750510334968567}, {\"epoch\": 10, \"type\": \"val_loss\", \"value\": 0.6736671328544617}, {\"epoch\": 11, \"type\": \"val_loss\", \"value\": 0.6722921133041382}, {\"epoch\": 12, \"type\": \"val_loss\", \"value\": 0.6710519194602966}, {\"epoch\": 13, \"type\": \"val_loss\", \"value\": 0.6697986125946045}, {\"epoch\": 14, \"type\": \"val_loss\", \"value\": 0.6676528453826904}, {\"epoch\": 15, \"type\": \"val_loss\", \"value\": 0.6659294962882996}, {\"epoch\": 16, \"type\": \"val_loss\", \"value\": 0.6644183397293091}, {\"epoch\": 17, \"type\": \"val_loss\", \"value\": 0.6620016694068909}, {\"epoch\": 18, \"type\": \"val_loss\", \"value\": 0.659735381603241}, {\"epoch\": 19, \"type\": \"val_loss\", \"value\": 0.6572684049606323}], \"data-3dd7fde33f5bb77aa17b186747c5d1e2\": [{\"epoch\": 0, \"type\": \"precision\", \"value\": 0.4936586320400238}, {\"epoch\": 1, \"type\": \"precision\", \"value\": 0.5240039825439453}, {\"epoch\": 2, \"type\": \"precision\", \"value\": 0.5571057200431824}, {\"epoch\": 3, \"type\": \"precision\", \"value\": 0.5770282745361328}, {\"epoch\": 4, \"type\": \"precision\", \"value\": 0.5758844017982483}, {\"epoch\": 5, \"type\": \"precision\", \"value\": 0.5890156626701355}, {\"epoch\": 6, \"type\": \"precision\", \"value\": 0.5838972330093384}, {\"epoch\": 7, \"type\": \"precision\", \"value\": 0.5879914164543152}, {\"epoch\": 8, \"type\": \"precision\", \"value\": 0.5876571536064148}, {\"epoch\": 9, \"type\": \"precision\", \"value\": 0.590468168258667}, {\"epoch\": 10, \"type\": \"precision\", \"value\": 0.5910171866416931}, {\"epoch\": 11, \"type\": \"precision\", \"value\": 0.5983051061630249}, {\"epoch\": 12, \"type\": \"precision\", \"value\": 0.597247838973999}, {\"epoch\": 13, \"type\": \"precision\", \"value\": 0.5999714732170105}, {\"epoch\": 14, \"type\": \"precision\", \"value\": 0.5983211994171143}, {\"epoch\": 15, \"type\": \"precision\", \"value\": 0.6053378582000732}, {\"epoch\": 16, \"type\": \"precision\", \"value\": 0.6100566387176514}, {\"epoch\": 17, \"type\": \"precision\", \"value\": 0.6091715097427368}, {\"epoch\": 18, \"type\": \"precision\", \"value\": 0.6137987971305847}, {\"epoch\": 19, \"type\": \"precision\", \"value\": 0.6147095561027527}, {\"epoch\": 0, \"type\": \"val_precision\", \"value\": 0.5020154118537903}, {\"epoch\": 1, \"type\": \"val_precision\", \"value\": 0.5387409329414368}, {\"epoch\": 2, \"type\": \"val_precision\", \"value\": 0.5567643046379089}, {\"epoch\": 3, \"type\": \"val_precision\", \"value\": 0.5634441375732422}, {\"epoch\": 4, \"type\": \"val_precision\", \"value\": 0.5649068355560303}, {\"epoch\": 5, \"type\": \"val_precision\", \"value\": 0.5645599961280823}, {\"epoch\": 6, \"type\": \"val_precision\", \"value\": 0.5652173757553101}, {\"epoch\": 7, \"type\": \"val_precision\", \"value\": 0.5673583149909973}, {\"epoch\": 8, \"type\": \"val_precision\", \"value\": 0.5669819712638855}, {\"epoch\": 9, \"type\": \"val_precision\", \"value\": 0.567905068397522}, {\"epoch\": 10, \"type\": \"val_precision\", \"value\": 0.5723370313644409}, {\"epoch\": 11, \"type\": \"val_precision\", \"value\": 0.5726900100708008}, {\"epoch\": 12, \"type\": \"val_precision\", \"value\": 0.5712515711784363}, {\"epoch\": 13, \"type\": \"val_precision\", \"value\": 0.573236882686615}, {\"epoch\": 14, \"type\": \"val_precision\", \"value\": 0.5798893570899963}, {\"epoch\": 15, \"type\": \"val_precision\", \"value\": 0.5834398865699768}, {\"epoch\": 16, \"type\": \"val_precision\", \"value\": 0.5792978405952454}, {\"epoch\": 17, \"type\": \"val_precision\", \"value\": 0.5850997567176819}, {\"epoch\": 18, \"type\": \"val_precision\", \"value\": 0.5922638177871704}, {\"epoch\": 19, \"type\": \"val_precision\", \"value\": 0.6015831232070923}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizeHistory(cnn_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 1s 3ms/step\n",
      "189/189 [==============================] - 0s 3ms/step\n",
      "\n",
      "Classifcation Report of Performance on Training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.63      0.63      7000\n",
      "           1       0.63      0.62      0.62      7109\n",
      "\n",
      "    accuracy                           0.62     14109\n",
      "   macro avg       0.63      0.63      0.62     14109\n",
      "weighted avg       0.63      0.62      0.62     14109\n",
      "\n",
      "\n",
      "\n",
      "* * * * * * * * * * \n",
      "\n",
      "Classifcation Report of Performance on Validation data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.61      0.61      3078\n",
      "           1       0.60      0.61      0.61      2969\n",
      "\n",
      "    accuracy                           0.61      6047\n",
      "   macro avg       0.61      0.61      0.61      6047\n",
      "weighted avg       0.61      0.61      0.61      6047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performanceReport(cnn, [X_train_enc, X_train_token], y_train, [X_val_enc, X_val_token], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following section contains the optimization and further evaluation of chosen models from the previous  section. The model(s) are optimized in hyperparameters and feature extraction. \n",
    "Currently, the models were trained using the tokenized data including stop words and the encoded categorical channel columns. Feature extraction is going to decide which features are needed to achieve the best results. \n",
    "\n",
    "Focus in optimization and feature extraction are to have less complicity combined with only using features contributing to improving a models overall performance.\n",
    "At the end, the best found model should be able to master the task of fake-news classification based on the test set which consist of the famous LIAR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
